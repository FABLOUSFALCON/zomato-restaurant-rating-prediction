{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77767993-b31b-4ce5-b7f8-42107810d660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:14.167345Z",
     "iopub.status.busy": "2025-09-16T14:10:14.167147Z",
     "iopub.status.idle": "2025-09-16T14:10:16.288347Z",
     "shell.execute_reply": "2025-09-16T14:10:16.287625Z",
     "shell.execute_reply.started": "2025-09-16T14:10:14.167328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 19:40:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ All libraries imported and configurations set successfully!\u001b[0m\n",
      "\u001b[32m2025-09-16 19:40:16\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSuccessfully loaded the NLP dataset from '../data/processed/zomato_nlp.parquet'.\u001b[0m\n",
      "\u001b[32m2025-09-16 19:40:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mDataFrame shape: (45187, 7)\u001b[0m\n",
      "\u001b[32m2025-09-16 19:40:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ All libraries imported and configurations set successfully!\u001b[0m\n",
      "\u001b[32m2025-09-16 19:40:16\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSuccessfully loaded the Geo dataset from '../data/processed/zomato_geo.parquet'.\u001b[0m\n",
      "\u001b[32m2025-09-16 19:40:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mDataFrame shape: (45187, 5)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>rate</th>\n",
       "      <th>location</th>\n",
       "      <th>listed_in_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>1st Floor, Annakuteera, 3rd Stage, Banashankar...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>10, 3rd Floor, Lakshmi Associates, Gandhi Baza...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Basavanagudi</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                            address  \\\n",
       "0                  Jalsa  942, 21st Main Road, 2nd Stage, Banashankari, ...   \n",
       "1         Spice Elephant  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...   \n",
       "2        San Churro Cafe  1112, Next to KIMS Medical College, 17th Cross...   \n",
       "3  Addhuri Udupi Bhojana  1st Floor, Annakuteera, 3rd Stage, Banashankar...   \n",
       "4          Grand Village  10, 3rd Floor, Lakshmi Associates, Gandhi Baza...   \n",
       "\n",
       "   rate      location listed_in_city  \n",
       "0   4.1  Banashankari   Banashankari  \n",
       "1   4.1  Banashankari   Banashankari  \n",
       "2   3.8  Banashankari   Banashankari  \n",
       "3   3.7  Banashankari   Banashankari  \n",
       "4   3.8  Basavanagudi   Banashankari  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. CORE LIBRARIES ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import folium  # For interactive mapping\n",
    "\n",
    "# --- 3. GEOSPATIAL LIBRARIES ---\n",
    "import geopy.geocoders\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. DATA HANDLING & ANALYSIS ---\n",
    "import pandas as pd\n",
    "from geopy.extra.rate_limiter import RateLimiter  # To avoid spamming the server\n",
    "from geopy.geocoders import Nominatim  # A popular free geocoding service\n",
    "# --- 1. CORE LIBRARIES ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# In your main setup cell, replace the old NLTK section with this:\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. DATA HANDLING & ANALYSIS ---\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- 3. NATURAL LANGUAGE PROCESSING (NLP) ---\n",
    "from textblob import TextBlob  # For easy sentiment analysis\n",
    "\n",
    "# This will now work because you've manually downloaded the data.\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# --- 4. UTILITIES ---\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ===================================================================\n",
    "#                      CONFIGURATION\n",
    "# ===================================================================\n",
    "# (Your standard, excellent configuration settings)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# ... etc. ...\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    colorize=True,\n",
    "    format=(\n",
    "        \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n",
    "        \"<level>{level: <8}</level> | \"\n",
    "        \"<level>{message}</level>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(\"✅ All libraries imported and configurations set successfully!\")\n",
    "\n",
    "# --- Load the NLP Dataset ---\n",
    "DATA_PATH = \"../data/processed/zomato_nlp.parquet\"\n",
    "try:\n",
    "    df_nlp = pd.read_parquet(DATA_PATH)\n",
    "    logger.success(f\"Successfully loaded the NLP dataset from '{DATA_PATH}'.\")\n",
    "    logger.info(f\"DataFrame shape: {df_nlp.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\n",
    "        f\"FATAL: The file was not found at '{DATA_PATH}'. Please ensure the path is correct.\"\n",
    "    )\n",
    "\n",
    "df_nlp.head()\n",
    "# --- 4. UTILITIES ---\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()  # Enable progress bars for pandas apply\n",
    "\n",
    "# ===================================================================\n",
    "#                      CONFIGURATION\n",
    "# ===================================================================\n",
    "# (Same pandas, plotting, and loguru settings as before)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# ... (rest of your standard config) ...\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    colorize=True,\n",
    "    format=(\n",
    "        \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n",
    "        \"<level>{level: <8}</level> | \"\n",
    "        \"<level>{message}</level>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(\"✅ All libraries imported and configurations set successfully!\")\n",
    "\n",
    "# --- Load the Geo Dataset ---\n",
    "DATA_PATH = \"../data/processed/zomato_geo.parquet\"\n",
    "try:\n",
    "    df_geo = pd.read_parquet(DATA_PATH)\n",
    "    logger.success(f\"Successfully loaded the Geo dataset from '{DATA_PATH}'.\")\n",
    "    logger.info(f\"DataFrame shape: {df_geo.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"FATAL: The file was not found at '{DATA_PATH}'.\")\n",
    "\n",
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be53a78e-df18-46dc-b876-3ea2c3b1a90d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:39:53.796854Z",
     "iopub.status.busy": "2025-09-10T18:39:53.795332Z",
     "iopub.status.idle": "2025-09-10T18:39:55.621851Z",
     "shell.execute_reply": "2025-09-10T18:39:55.621094Z",
     "shell.execute_reply.started": "2025-09-10T18:39:53.796833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Setting up the environment ---\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 1: Identifying unique restaurants and loading cache...\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 2: Creating a sample of 10 for the test run...\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 3: Running the batch geocoding function on the sample...\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Starting Azure Maps Asynchronous Batch Geocoding ---\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAll sample addresses are already in the cache.\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mCache saved to 'azure_batch_geocache_test.json'.\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 4: Preparing sample results for verification...\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Verifying coordinates with Manual REST API Batch Reverse Geocoding ---\u001b[0m\n",
      "\u001b[32m2025-09-11 00:09:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Geocoding Verification Report ---\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>verified_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>63, SH 35, Vinayaka Layout, Bengaluru, Karnata...</td>\n",
       "      <td>819, Whitefield Main Road, Vinayaka Layout, Wh...</td>\n",
       "      <td>12.969086</td>\n",
       "      <td>77.749974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>303, 5th Main, 100 Feet Road, Indiranagar, Ban...</td>\n",
       "      <td>32, 5th Main Road, Indira Nagar I Stage, Benga...</td>\n",
       "      <td>12.979962</td>\n",
       "      <td>77.640713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>Arya Hub Mall, ITPL Main Road, Whitefield, Ban...</td>\n",
       "      <td>Whitefield Main Road, Prashanth Extension, Whi...</td>\n",
       "      <td>12.983661</td>\n",
       "      <td>77.752186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>602, 4th Cross, 2nd Block, HRBR Layout, Kalyan...</td>\n",
       "      <td>2, 4th D Cross Road, HRBR Layout 3rd Block, Ka...</td>\n",
       "      <td>13.025616</td>\n",
       "      <td>77.633580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9839</th>\n",
       "      <td>5, 1st Main, 2nd Block, 3rd Stage, Basaveshwar...</td>\n",
       "      <td>Vinayaka Layout Road, Kodanda Reddy Layout, Du...</td>\n",
       "      <td>13.011113</td>\n",
       "      <td>77.670645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>80, BDA Complex, 2nd Stage, Banashankari, Bang...</td>\n",
       "      <td>645, 24th Cross Road, Banashankari Stage 2, Be...</td>\n",
       "      <td>12.924435</td>\n",
       "      <td>77.565474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>3, Outer Ring Road, Opposite More Megastore, M...</td>\n",
       "      <td>Mahadevapura Outer Ring Road, Konadas Pura, Ma...</td>\n",
       "      <td>12.989707</td>\n",
       "      <td>77.688775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>36, Vittal Mallya Road, Lavelle Road, Bangalore</td>\n",
       "      <td>38-2, Vittal Mallya Road, Shanthala Nagar, Ben...</td>\n",
       "      <td>12.971137</td>\n",
       "      <td>77.597785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>Nageshwar Rao Building, 3rd Crass Kamadhenu, L...</td>\n",
       "      <td>159, 3rd Cross Road, Indira Nagar Layout, Maha...</td>\n",
       "      <td>12.996587</td>\n",
       "      <td>77.686737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>2nd cross, Behind Anjaneya Temple, Opposite Pr...</td>\n",
       "      <td>6-14, 2nd Cross Road, Madivala, BTM Layout I S...</td>\n",
       "      <td>12.922288</td>\n",
       "      <td>77.618638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 address  \\\n",
       "10347  63, SH 35, Vinayaka Layout, Bengaluru, Karnata...   \n",
       "6070   303, 5th Main, 100 Feet Road, Indiranagar, Ban...   \n",
       "10109  Arya Hub Mall, ITPL Main Road, Whitefield, Ban...   \n",
       "7637   602, 4th Cross, 2nd Block, HRBR Layout, Kalyan...   \n",
       "9839   5, 1st Main, 2nd Block, 3rd Stage, Basaveshwar...   \n",
       "33     80, BDA Complex, 2nd Stage, Banashankari, Bang...   \n",
       "9281   3, Outer Ring Road, Opposite More Megastore, M...   \n",
       "2970     36, Vittal Mallya Road, Lavelle Road, Bangalore   \n",
       "9129   Nageshwar Rao Building, 3rd Crass Kamadhenu, L...   \n",
       "4734   2nd cross, Behind Anjaneya Temple, Opposite Pr...   \n",
       "\n",
       "                                        verified_address   latitude  longitude  \n",
       "10347  819, Whitefield Main Road, Vinayaka Layout, Wh...  12.969086  77.749974  \n",
       "6070   32, 5th Main Road, Indira Nagar I Stage, Benga...  12.979962  77.640713  \n",
       "10109  Whitefield Main Road, Prashanth Extension, Whi...  12.983661  77.752186  \n",
       "7637   2, 4th D Cross Road, HRBR Layout 3rd Block, Ka...  13.025616  77.633580  \n",
       "9839   Vinayaka Layout Road, Kodanda Reddy Layout, Du...  13.011113  77.670645  \n",
       "33     645, 24th Cross Road, Banashankari Stage 2, Be...  12.924435  77.565474  \n",
       "9281   Mahadevapura Outer Ring Road, Konadas Pura, Ma...  12.989707  77.688775  \n",
       "2970   38-2, Vittal Mallya Road, Shanthala Nagar, Ben...  12.971137  77.597785  \n",
       "9129   159, 3rd Cross Road, Indira Nagar Layout, Maha...  12.996587  77.686737  \n",
       "4734   6-14, 2nd Cross Road, Madivala, BTM Layout I S...  12.922288  77.618638  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.maps.search import MapsSearchClient\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- STEP 1: SETUP & CONFIGURATION ---\n",
    "logger.info(\"--- Setting up the environment ---\")\n",
    "load_dotenv()\n",
    "AZURE_MAPS_KEY = os.getenv(\"AZURE_MAPS_KEY\")\n",
    "if not AZURE_MAPS_KEY:\n",
    "    logger.error(\"FATAL: AZURE_MAPS_KEY not found.\")\n",
    "    maps_search_client = None\n",
    "else:\n",
    "    maps_search_client = MapsSearchClient(credential=AzureKeyCredential(AZURE_MAPS_KEY))\n",
    "\n",
    "\n",
    "# --- BATCH GEOCODING FUNCTION (UNCHANGED) ---\n",
    "def geocode_with_azure_batch(df_to_geocode: pd.DataFrame, cache: dict) -> dict:\n",
    "    if not AZURE_MAPS_KEY:\n",
    "        return cache\n",
    "    logger.info(\"--- Starting Azure Maps Asynchronous Batch Geocoding ---\")\n",
    "    addresses_to_geocode_new = df_to_geocode[\n",
    "        ~df_to_geocode[\"address\"].isin(cache.keys())\n",
    "    ]\n",
    "    if addresses_to_geocode_new.empty:\n",
    "        logger.success(\"All sample addresses are already in the cache.\")\n",
    "        return cache\n",
    "    logger.info(\n",
    "        f\"Preparing a batch of {len(addresses_to_geocode_new)} new addresses...\"\n",
    "    )\n",
    "    batch_items = []\n",
    "    for index, row in addresses_to_geocode_new.iterrows():\n",
    "        query_text = f\"{row['name']}, {row['address']}\"\n",
    "        batch_query = f\"?query={requests.utils.quote(query_text)}&countrySet=IN\"\n",
    "        batch_items.append({\"query\": batch_query})\n",
    "    submit_url = f\"https://atlas.microsoft.com/search/fuzzy/batch/json?api-version=1.0&subscription-key={AZURE_MAPS_KEY}\"\n",
    "    logger.info(f\"Submitting {len(batch_items)} queries...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            submit_url, json={\"batchItems\": batch_items}, timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to submit batch job: {e}\")\n",
    "        return cache\n",
    "    if response.status_code == 202:\n",
    "        status_url = response.headers[\"Location\"]\n",
    "        logger.success(\"Batch job accepted.\")\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"Batch submission failed with status {response.status_code}: {response.text}\"\n",
    "        )\n",
    "        return cache\n",
    "    logger.info(\"Polling for results...\")\n",
    "    while True:\n",
    "        try:\n",
    "            result_response = requests.get(status_url, timeout=30)\n",
    "            result_response.raise_for_status()\n",
    "            if result_response.status_code == 202:\n",
    "                logger.info(\"...processing, waiting 5s...\")\n",
    "                time.sleep(5)\n",
    "            elif result_response.status_code == 200:\n",
    "                logger.success(\"Processing complete.\")\n",
    "                batch_result = result_response.json()\n",
    "                break\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"Polling failed with status {result_response.status_code}\"\n",
    "                )\n",
    "                return cache\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error while polling: {e}\")\n",
    "            return cache\n",
    "    logger.info(\"Parsing results and updating cache...\")\n",
    "    for i, item in enumerate(batch_result[\"batchItems\"]):\n",
    "        original_address = addresses_to_geocode_new.iloc[i][\"address\"]\n",
    "        if item[\"statusCode\"] == 200 and item[\"response\"].get(\"results\"):\n",
    "            first_result = item[\"response\"][\"results\"][0]\n",
    "            lat = first_result[\"position\"][\"lat\"]\n",
    "            lon = first_result[\"position\"][\"lon\"]\n",
    "            cache[original_address] = {\"latitude\": lat, \"longitude\": lon}\n",
    "        else:\n",
    "            cache[original_address] = {\"latitude\": None, \"longitude\": None}\n",
    "    logger.success(\"Cache updated.\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "def verify_geocoding_with_rest_api(df_with_coords: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a BATCH reverse geocode using direct REST API calls,\n",
    "    following the official documentation precisely.\n",
    "    \"\"\"\n",
    "    if not AZURE_MAPS_KEY:\n",
    "        logger.warning(\"Azure key not available. Skipping verification.\")\n",
    "        df_with_coords[\"verified_address\"] = \"Verification Skipped\"\n",
    "        return df_with_coords\n",
    "\n",
    "    logger.info(\n",
    "        \"--- Verifying coordinates with Manual REST API Batch Reverse Geocoding ---\"\n",
    "    )\n",
    "\n",
    "    df_to_verify = df_with_coords.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    if df_to_verify.empty:\n",
    "        df_with_coords[\"verified_address\"] = \"Original geocode failed\"\n",
    "        return df_with_coords\n",
    "\n",
    "    # --- 1. Construct the POST Body ---\n",
    "    # As per the documentation, the query is \"?query=lat,lon\"\n",
    "    batch_items = []\n",
    "    for index, row in df_to_verify.iterrows():\n",
    "        query_str = f\"?query={row['latitude']},{row['longitude']}\"\n",
    "        batch_items.append({\"query\": query_str})\n",
    "\n",
    "    # --- 2. Submit the Asynchronous Job ---\n",
    "    submit_url = f\"https://atlas.microsoft.com/search/address/reverse/batch/json?api-version=1.0&subscription-key={AZURE_MAPS_KEY}\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            submit_url, json={\"batchItems\": batch_items}, timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to submit reverse geocode batch job: {e}\")\n",
    "        df_with_coords[\"verified_address\"] = \"Error on Submit\"\n",
    "        return df_with_coords\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        status_url = response.headers[\"Location\"]\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"Reverse geocode submission failed with status {response.status_code}\"\n",
    "        )\n",
    "        df_with_coords[\"verified_address\"] = \"Error on Submit\"\n",
    "        return df_with_coords\n",
    "\n",
    "    # --- 3. Poll for Results ---\n",
    "    while True:\n",
    "        try:\n",
    "            result_response = requests.get(status_url, timeout=30)\n",
    "            result_response.raise_for_status()\n",
    "            if result_response.status_code == 200:\n",
    "                batch_result = result_response.json()\n",
    "                break\n",
    "            time.sleep(2)  # Wait 2 seconds between polls\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error while polling reverse geocode results: {e}\")\n",
    "            df_with_coords[\"verified_address\"] = \"Error on Poll\"\n",
    "            return df_with_coords\n",
    "\n",
    "    # --- 4. Parse Results ---\n",
    "    verified_addresses = {}\n",
    "    for i, item in enumerate(batch_result[\"batchItems\"]):\n",
    "        original_index = df_to_verify.index[i]\n",
    "        if item[\"statusCode\"] == 200 and item[\"response\"][\"addresses\"]:\n",
    "            verified_addresses[original_index] = item[\"response\"][\"addresses\"][0][\n",
    "                \"address\"\n",
    "            ][\"freeformAddress\"]\n",
    "        else:\n",
    "            verified_addresses[original_index] = \"Reverse geocode failed\"\n",
    "\n",
    "    df_with_coords[\"verified_address\"] = df_with_coords.index.map(\n",
    "        verified_addresses\n",
    "    ).fillna(\"Not Verified (No Coords)\")\n",
    "    return df_with_coords\n",
    "\n",
    "\n",
    "# --- SCRIPT EXECUTION ---\n",
    "logger.info(\"Step 1: Identifying unique restaurants and loading cache...\")\n",
    "df_unique_restaurants = (\n",
    "    df_geo[[\"name\", \"address\", \"location\"]].drop_duplicates().reset_index(drop=True)\n",
    ")\n",
    "CACHE_FILE = \"azure_batch_geocache_test.json\"\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "logger.info(\"Step 2: Creating a sample of 10 for the test run...\")\n",
    "df_sample_to_process = df_unique_restaurants.sample(n=10, random_state=42)\n",
    "\n",
    "logger.info(\"Step 3: Running the batch geocoding function on the sample...\")\n",
    "updated_cache = geocode_with_azure_batch(df_sample_to_process, cache)\n",
    "\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(updated_cache, f, indent=2)\n",
    "logger.success(f\"Cache saved to '{CACHE_FILE}'.\")\n",
    "\n",
    "# --- Step 4: Prepare the sample results for verification ---\n",
    "logger.info(\"Step 4: Preparing sample results for verification...\")\n",
    "df_sample_results = df_sample_to_process.copy()\n",
    "lat_map = {addr: data.get(\"latitude\") for addr, data in updated_cache.items()}\n",
    "lon_map = {addr: data.get(\"longitude\") for addr, data in updated_cache.items()}\n",
    "df_sample_results[\"latitude\"] = df_sample_results[\"address\"].map(lat_map)\n",
    "df_sample_results[\"longitude\"] = df_sample_results[\"address\"].map(lon_map)\n",
    "\n",
    "# --- Step 5: Run the NEW, DOCUMENTATION-DRIVEN VERIFICATION function ---\n",
    "df_verified_results = verify_geocoding_with_rest_api(df_sample_results)\n",
    "\n",
    "# --- FINAL REPORT ---\n",
    "logger.info(\"--- Geocoding Verification Report ---\")\n",
    "display(df_verified_results[[\"address\", \"verified_address\", \"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda9430e-5817-4fd7-b28c-92bd6f7293d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:39:29.339443Z",
     "iopub.status.busy": "2025-09-16T11:39:29.339259Z",
     "iopub.status.idle": "2025-09-16T11:39:30.310332Z",
     "shell.execute_reply": "2025-09-16T11:39:30.309605Z",
     "shell.execute_reply.started": "2025-09-16T11:39:29.339426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5875, 0.2518, 0.1725],\n",
      "        [0.5259, 0.8692, 0.8414],\n",
      "        [0.5028, 0.5129, 0.8821],\n",
      "        [0.4729, 0.2335, 0.6795],\n",
      "        [0.7349, 0.3373, 0.9296]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac3e1e1-7b76-4705-baeb-e163d070c3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:45:06.267314Z",
     "iopub.status.busy": "2025-09-16T11:45:06.267071Z",
     "iopub.status.idle": "2025-09-16T11:45:08.506484Z",
     "shell.execute_reply": "2025-09-16T11:45:08.505865Z",
     "shell.execute_reply.started": "2025-09-16T11:45:06.267296Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2068fc35-23c5-4626-a9ce-c2cba32bb1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:45:08.507370Z",
     "iopub.status.busy": "2025-09-16T11:45:08.507128Z",
     "iopub.status.idle": "2025-09-16T11:45:10.409933Z",
     "shell.execute_reply": "2025-09-16T11:45:10.409275Z",
     "shell.execute_reply.started": "2025-09-16T11:45:08.507356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function transformers.pipelines.pipeline(task: Optional[str] = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, processor: Union[str, transformers.processing_utils.ProcessorMixin, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map: Union[str, dict[str, Union[int, str]], NoneType] = None, dtype: Union[str, ForwardRef('torch.dtype'), NoneType] = 'auto', trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> transformers.pipelines.base.Pipeline>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb4d9f9-24a7-4d71-97a9-29f1912fae83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:44:57.323239Z",
     "iopub.status.busy": "2025-09-16T11:44:57.323010Z",
     "iopub.status.idle": "2025-09-16T11:44:57.556537Z",
     "shell.execute_reply": "2025-09-16T11:44:57.555899Z",
     "shell.execute_reply.started": "2025-09-16T11:44:57.323211Z"
    }
   },
   "outputs": [],
   "source": [
    "import triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529f8d47-c9d0-4f50-912c-cf178c2fc179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:44:57.564615Z",
     "iopub.status.busy": "2025-09-16T11:44:57.564479Z",
     "iopub.status.idle": "2025-09-16T11:44:57.571162Z",
     "shell.execute_reply": "2025-09-16T11:44:57.570510Z",
     "shell.execute_reply.started": "2025-09-16T11:44:57.564602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CompilationError',\n",
       " 'Config',\n",
       " 'InterpreterError',\n",
       " 'JITFunction',\n",
       " 'KernelInterface',\n",
       " 'MockTensor',\n",
       " 'OutOfResources',\n",
       " 'TensorWrapper',\n",
       " 'TritonError',\n",
       " '_C',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'autotune',\n",
       " 'backends',\n",
       " 'cdiv',\n",
       " 'compile',\n",
       " 'compiler',\n",
       " 'errors',\n",
       " 'heuristics',\n",
       " 'jit',\n",
       " 'language',\n",
       " 'next_power_of_2',\n",
       " 'reinterpret',\n",
       " 'runtime',\n",
       " 'testing',\n",
       " 'tools']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0aac17-0ce5-4af5-af7e-da49f5897244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:27:06.571228Z",
     "iopub.status.busy": "2025-09-16T13:27:06.571069Z",
     "iopub.status.idle": "2025-09-16T13:27:08.775776Z",
     "shell.execute_reply": "2025-09-16T13:27:08.775059Z",
     "shell.execute_reply.started": "2025-09-16T13:27:06.571213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 18:57:08.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m--- PyTorch GPU Diagnostic ---\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mPyTorch Version: 2.8.0+cu129\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mIs CUDA available to PyTorch? -> True\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.771\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[32m\u001b[1mGPU DETECTED SUCCESSFULLY!\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mGPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\u001b[0m\n",
      "\u001b[32m2025-09-16 18:57:08.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mPyTorch was compiled with CUDA Version: 12.9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from loguru import logger\n",
    "\n",
    "logger.info(\"--- PyTorch GPU Diagnostic ---\")\n",
    "\n",
    "# 1. Check if PyTorch is installed\n",
    "logger.info(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# 2. Check if CUDA is available TO PYTORCH\n",
    "is_available = torch.cuda.is_available()\n",
    "logger.info(f\"Is CUDA available to PyTorch? -> {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    # 3. If it is available, get details about the GPU\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    cuda_version_pytorch = torch.version.cuda\n",
    "\n",
    "    logger.success(\"GPU DETECTED SUCCESSFULLY!\")\n",
    "    logger.info(f\"Number of GPUs: {gpu_count}\")\n",
    "    logger.info(f\"GPU Name: {gpu_name}\")\n",
    "    logger.info(f\"PyTorch was compiled with CUDA Version: {cuda_version_pytorch}\")\n",
    "else:\n",
    "    logger.error(\"GPU NOT DETECTED BY PYTORCH.\")\n",
    "    logger.warning(\n",
    "        \"This is likely due to an installation mismatch between your NVIDIA driver, the CUDA toolkit version PyTorch was compiled with, and the PyTorch version itself.\"\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Recommendation: Re-run the official PyTorch installation command from their website for your specific CUDA version.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14523e7-a488-4dd2-bd62-88d0f7c5a44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:57:16.110835Z",
     "iopub.status.busy": "2025-09-16T13:57:16.110457Z",
     "iopub.status.idle": "2025-09-16T13:57:23.397883Z",
     "shell.execute_reply": "2025-09-16T13:57:23.395691Z",
     "shell.execute_reply.started": "2025-09-16T13:57:16.110813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-09-16 19:27:19.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1m--- Loading Data from VADER Sentiment Checkpoint for Transformer ---\u001b[0m\n",
      "\u001b[32m2025-09-16 19:27:20.104\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m65\u001b[0m - \u001b[32m\u001b[1mCheckpoint loaded.\u001b[0m\n",
      "\u001b[32m2025-09-16 19:27:20.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_transformer_sentiment\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m--- Initializing the Advanced Multilingual Sentiment Model (XLM-RoBERTa) ---\u001b[0m\n",
      "\u001b[32m2025-09-16 19:27:23.162\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_transformer_sentiment\u001b[0m:\u001b[36m23\u001b[0m - \u001b[32m\u001b[1mMultilingual sentiment model initialized successfully ON GPU.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08c03aa4e2a4bd3924250dde0409b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 19:27:23.204\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_transformer_sentiment\u001b[0m:\u001b[36m58\u001b[0m - \u001b[32m\u001b[1mAdvanced sentiment analysis complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification of Transformer Sentiment ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rate</th>\n",
       "      <th>transformer_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  rate  transformer_sentiment\n",
       "0                  Jalsa   4.1                  0.069\n",
       "1         Spice Elephant   4.1                  0.069\n",
       "2        San Churro Cafe   3.8                  0.069\n",
       "3  Addhuri Udupi Bhojana   3.7                  0.069\n",
       "4          Grand Village   3.8                  0.069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "\n",
    "def generate_transformer_sentiment(df: pd.DataFrame, review_col: str = 'reviews_list') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the average sentiment score using a Transformer model.\n",
    "    Includes the 'trust_remote_code' fix.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Initializing the Advanced Multilingual Sentiment Model (XLM-RoBERTa) ---\")\n",
    "    \n",
    "    try:\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=0, # Use GPU\n",
    "        )\n",
    "        logger.success(\"Multilingual sentiment model initialized successfully ON GPU.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize sentiment pipeline on GPU. Error: {e}\")\n",
    "        logger.warning(\"Falling back to CPU. This will be very slow.\")\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=-1,\n",
    "        )\n",
    "\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # The get_avg_sentiment sub-function remains the same\n",
    "    def get_avg_sentiment(review_array):\n",
    "        if review_array is None or not hasattr(review_array, '__iter__'):\n",
    "            return 0.069\n",
    "        # Step 2: Now that we know it's iterable, we can safely check its length.\n",
    "        if len(review_array) == 0:\n",
    "            return 0.067\n",
    "        review_texts = [review[1] for review in review_array if len(review) == 2 and isinstance(review[1], str)]\n",
    "        if not review_texts: return 0.999\n",
    "        sentiments = []\n",
    "        try:\n",
    "            results = sentiment_pipeline(review_texts, truncation=True, max_length=512)\n",
    "            for result in results:\n",
    "                if result['label'] == 'Positive': sentiments.append(1 * result['score'])\n",
    "                elif result['label'] == 'Negative': sentiments.append(-1 * result['score'])\n",
    "                else: sentiments.append(0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not process a batch of reviews. Error: {e}\")\n",
    "            return 0.0\n",
    "        return np.mean(sentiments) if sentiments else 0.107\n",
    "\n",
    "    # Apply the function to the entire DataFrame\n",
    "    df_out['transformer_sentiment'] = df_out[review_col].progress_apply(get_avg_sentiment)\n",
    "    logger.success(\"Advanced sentiment analysis complete.\")\n",
    "    return df_out\n",
    "\n",
    "# --- Execute Transformer Analysis ---\n",
    "logger.info(\"--- Loading Data from VADER Sentiment Checkpoint for Transformer ---\")\n",
    "CHECKPOINT_PATH = \"../data/processed/zomato_nlp_with_sentiment.parquet\"\n",
    "df_for_transformer = pd.read_parquet(CHECKPOINT_PATH)\n",
    "logger.success(\"Checkpoint loaded.\")\n",
    "\n",
    "# We run this on the ORIGINAL 'reviews_list' column for maximum accuracy\n",
    "df_with_transformer = generate_transformer_sentiment(df_for_transformer)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification of Transformer Sentiment ---\")\n",
    "display(df_with_transformer[['name', 'rate', 'transformer_sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d75e36d-2bf2-4a2d-8f35-de297df843b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:58:39.083364Z",
     "iopub.status.busy": "2025-09-16T13:58:39.083106Z",
     "iopub.status.idle": "2025-09-16T13:58:39.089041Z",
     "shell.execute_reply": "2025-09-16T13:58:39.088165Z",
     "shell.execute_reply.started": "2025-09-16T13:58:39.083346Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "45182    None\n",
       "45183    None\n",
       "45184    None\n",
       "45185    None\n",
       "45186    None\n",
       "Name: reviews_list, Length: 45187, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_transformer[\"reviews_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0627a11-b09b-4de4-9490-9dca0edde663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:28:23.528004Z",
     "iopub.status.busy": "2025-09-16T14:28:23.527629Z",
     "iopub.status.idle": "2025-09-16T15:36:27.939114Z",
     "shell.execute_reply": "2025-09-16T15:36:27.933324Z",
     "shell.execute_reply.started": "2025-09-16T14:28:23.527983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 19:58:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Starting from a Clean Slate ---\u001b[0m\n",
      "\u001b[32m2025-09-16 19:58:24\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSuccessfully loaded the ORIGINAL NLP dataset from '../data/processed/zomato_nlp.parquet'.\u001b[0m\n",
      "\u001b[32m2025-09-16 19:58:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mDataFrame shape: (45187, 7)\u001b[0m\n",
      "\u001b[32m2025-09-16 19:58:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Initializing the Advanced Multilingual Sentiment Model (Online) ---\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 19:58:31\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mMultilingual sentiment model initialized successfully ON GPU.\u001b[0m\n",
      "\u001b[32m2025-09-16 19:58:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mApplying sentiment analysis to the full dataset. This will take time...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a30491a3e2f4303a02af909fc4bf049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_34635/1710137661.py\", line 87, in <module>\n",
      "    df_with_transformer = generate_transformer_sentiment_definitive(df_nlp_source)\n",
      "  File \"/tmp/ipykernel_34635/1710137661.py\", line 79, in generate_transformer_sentiment_definitive\n",
      "    df_out['transformer_sentiment'] = df_out[review_col].progress_apply(get_avg_sentiment)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/tqdm/std.py\", line 917, in inner\n",
      "    return getattr(df, df_function)(wrapper, **kwargs)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/pandas/core/series.py\", line 4935, in apply\n",
      "    ).apply()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/pandas/core/apply.py\", line 1422, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/pandas/core/apply.py\", line 1502, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/pandas/core/base.py\", line 925, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/tqdm/std.py\", line 912, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_34635/1710137661.py\", line 67, in get_avg_sentiment\n",
      "    results = sentiment_pipeline(review_texts, truncation=True, max_length=512, batch_size=16)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n",
      "    result = super().__call__(*inputs, **kwargs)\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 21:06:23\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAdvanced sentiment analysis complete.\u001b[0m\n",
      "\n",
      "--- Verification of Transformer Sentiment ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rate</th>\n",
       "      <th>transformer_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  rate  transformer_sentiment\n",
       "0                  Jalsa   4.1                    0.0\n",
       "1         Spice Elephant   4.1                    0.0\n",
       "2        San Churro Cafe   3.8                    0.0\n",
       "3  Addhuri Udupi Bhojana   3.7                    0.0\n",
       "4          Grand Village   3.8                    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 21:06:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExporting final NLP features to '../data/processed/zomato_nlp_features_final.parquet'...\u001b[0m\n",
      "\u001b[32m2025-09-16 21:06:27\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mFinal NLP features saved successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- STEP 1: LOAD THE ORIGINAL, TRUSTWORTHY DATA ---\n",
    "logger.info(\"--- Starting from a Clean Slate ---\")\n",
    "DATA_PATH = \"../data/processed/zomato_nlp.parquet\"\n",
    "try:\n",
    "    df_nlp_source = pd.read_parquet(DATA_PATH)\n",
    "    logger.success(f\"Successfully loaded the ORIGINAL NLP dataset from '{DATA_PATH}'.\")\n",
    "    logger.info(f\"DataFrame shape: {df_nlp_source.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"FATAL: The file was not found at '{DATA_PATH}'. Cannot proceed.\")\n",
    "    # In a real script, we would stop here.\n",
    "    # For the notebook, create an empty df to prevent further errors.\n",
    "    df_nlp_source = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- STEP 2: THE DEFINITIVE SENTIMENT ANALYSIS FUNCTION ---\n",
    "def generate_transformer_sentiment_definitive(df: pd.DataFrame, review_col: str = 'reviews_list') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The final, definitive, robust version for calculating Transformer sentiment.\n",
    "    It loads the model online and handles all known data structure issues.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Input DataFrame is empty. Skipping sentiment analysis.\")\n",
    "        return df\n",
    "\n",
    "    logger.info(\"--- Initializing the Advanced Multilingual Sentiment Model (Online) ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model directly from Hugging Face Hub\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=0 # Use GPU\n",
    "        )\n",
    "        logger.success(\"Multilingual sentiment model initialized successfully ON GPU.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize sentiment pipeline on GPU. Error: {e}\")\n",
    "        logger.warning(\"Falling back to CPU. This will be very slow.\")\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=-1\n",
    "        )\n",
    "\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    def get_avg_sentiment(review_array):\n",
    "        # The most robust guard clause\n",
    "        if review_array is None or not hasattr(review_array, '__len__') or len(review_array) == 0:\n",
    "            return 0.696969\n",
    "        \n",
    "        # Correctly parse the array-of-arrays structure\n",
    "        review_texts = [review[1] for review in review_array if len(review) == 2 and isinstance(review[1], str)]\n",
    "        if not review_texts: \n",
    "            return 0.06789\n",
    "        \n",
    "        sentiments = []\n",
    "        try:\n",
    "            # Use a larger batch size for GPU efficiency\n",
    "            results = sentiment_pipeline(review_texts, truncation=True, max_length=512, batch_size=16)\n",
    "            for result in results:\n",
    "                if result['label'] == 'Positive': sentiments.append(1 * result['score'])\n",
    "                elif result['label'] == 'Negative': sentiments.append(-1 * result['score'])\n",
    "                else: sentiments.append(0)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not process a batch of reviews. Error: {e}\")\n",
    "            return 0.012345\n",
    "            \n",
    "        return np.mean(sentiments) if sentiments else 0.0234567\n",
    "\n",
    "    logger.info(\"Applying sentiment analysis to the full dataset. This will take time...\")\n",
    "    df_out['transformer_sentiment'] = df_out[review_col].progress_apply(get_avg_sentiment)\n",
    "    logger.success(\"Advanced sentiment analysis complete.\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# --- STEP 3: EXECUTE THE FULL PIPELINE ---\n",
    "# We run this on our clean, freshly loaded source DataFrame.\n",
    "# We will run the full dataset this time.\n",
    "df_with_transformer = generate_transformer_sentiment_definitive(df_nlp_source)\n",
    "\n",
    "\n",
    "# --- STEP 4: VERIFICATION & FINAL EXPORT ---\n",
    "if 'transformer_sentiment' in df_with_transformer.columns:\n",
    "    print(\"\\n--- Verification of Transformer Sentiment ---\")\n",
    "    display(df_with_transformer[['name', 'rate', 'transformer_sentiment']].head())\n",
    "    \n",
    "    # Let's save this hard-earned result!\n",
    "    FINAL_NLP_PATH = \"../data/processed/zomato_nlp_features_final.parquet\"\n",
    "    logger.info(f\"Exporting final NLP features to '{FINAL_NLP_PATH}'...\")\n",
    "    df_with_transformer.to_parquet(FINAL_NLP_PATH, index=False)\n",
    "    logger.success(\"Final NLP features saved successfully.\")\n",
    "else:\n",
    "    logger.error(\"Sentiment analysis did not complete successfully. No file was saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9e7e8c-6147-4dd8-bdb2-34f2aea15071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:18:21.259747Z",
     "iopub.status.busy": "2025-09-16T14:18:21.259509Z",
     "iopub.status.idle": "2025-09-16T14:18:21.265476Z",
     "shell.execute_reply": "2025-09-16T14:18:21.264854Z",
     "shell.execute_reply.started": "2025-09-16T14:18:21.259729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "45182    None\n",
       "45183    None\n",
       "45184    None\n",
       "45185    None\n",
       "45186    None\n",
       "Name: reviews_list, Length: 45187, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp_source[\"reviews_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b07671-435c-4c95-b54c-4333f2f0ae64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
