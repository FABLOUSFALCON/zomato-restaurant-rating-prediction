{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042a36d7-ce45-4090-9b6c-6a4636dc2067",
   "metadata": {},
   "source": [
    "# Zomato Bangalore Restaurants: 05 - NLP Feature Engineering\n",
    "\n",
    "**Author:** Puneet Kumar Mishra\n",
    "**Date:** 14-09-2025\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "This notebook is dedicated to extracting valuable, predictive features from the unstructured text data in our dataset. The primary goal is to convert the raw text from `reviews_list`, `menu_item`, `cuisines`, and `dish_liked` into meaningful numerical signals that our machine learning model can understand.\n",
    "\n",
    "This is a critical phase where we move beyond structured data and into the nuanced world of customer sentiment and restaurant offerings.\n",
    "\n",
    "### Key Features to Engineer:\n",
    "\n",
    "1.  **Simple Count-Based Features:** We will start by creating simple but powerful features like `review_count` and `menu_item_count`.\n",
    "2.  **Sentiment Analysis:** We will analyze the sentiment of all reviews for each restaurant to create an `avg_sentiment_score`. This has the potential to be a very strong predictor of the overall `rate`.\n",
    "3.  **Advanced Text Features (TF-IDF):** For high-cardinality text like `dish_liked` and `menu_item`, we will use TF-IDF vectorization to identify \"signature\" dishes or menu themes that are characteristic of high or low-rated restaurants.\n",
    "\n",
    "The final output will be a new dataset containing these engineered NLP features, ready to be merged with our main tabular and geo datasets for the final modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3eda49b-6391-4f6e-a1dd-27d61096a5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:45:23.076928Z",
     "iopub.status.busy": "2025-09-16T14:45:23.076693Z",
     "iopub.status.idle": "2025-09-16T14:45:26.897727Z",
     "shell.execute_reply": "2025-09-16T14:45:26.897054Z",
     "shell.execute_reply.started": "2025-09-16T14:45:23.076910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:15:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m✅ All libraries imported and configurations set successfully!\u001b[0m\n",
      "\u001b[32m2025-09-16 20:15:26\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSuccessfully loaded the NLP dataset from '../data/processed/zomato_nlp.parquet'.\u001b[0m\n",
      "\u001b[32m2025-09-16 20:15:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mDataFrame shape: (45187, 7)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>rate</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>dish_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[[Rated 2.0, RATED\\n  Its a restaurant near to...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Chinese, Mughlai, North Indian]</td>\n",
       "      <td>[Dum Biryani, Lunch Buffet, Masala Papad, Pane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[[Rated 2.0, RATED\\n  I had a very bad experie...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Chinese, North Indian, Thai]</td>\n",
       "      <td>[Chicken Biryani, Chocolate Nirvana, Dum Birya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[[Rated 1.0, RATED\\n  Cockroaches !! I Repeat ...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[Cafe, Italian, Mexican]</td>\n",
       "      <td>[Cannelloni, Churros, Hot Chocolate, Minestron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>1st Floor, Annakuteera, 3rd Stage, Banashankar...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>[[Rated 1.5, RATED\\n  The food was not satisfa...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[North Indian, South Indian]</td>\n",
       "      <td>[Masala Dosa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>10, 3rd Floor, Lakshmi Associates, Gandhi Baza...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[[Rated 4.0, RATED\\n  Great service, overwhelm...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[North Indian, Rajasthani]</td>\n",
       "      <td>[Gol Gappe, Panipuri]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                            address  \\\n",
       "0                  Jalsa  942, 21st Main Road, 2nd Stage, Banashankari, ...   \n",
       "1         Spice Elephant  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...   \n",
       "2        San Churro Cafe  1112, Next to KIMS Medical College, 17th Cross...   \n",
       "3  Addhuri Udupi Bhojana  1st Floor, Annakuteera, 3rd Stage, Banashankar...   \n",
       "4          Grand Village  10, 3rd Floor, Lakshmi Associates, Gandhi Baza...   \n",
       "\n",
       "   rate                                       reviews_list  menu_item  \\\n",
       "0   4.1  [[Rated 2.0, RATED\\n  Its a restaurant near to...  [Unknown]   \n",
       "1   4.1  [[Rated 2.0, RATED\\n  I had a very bad experie...  [Unknown]   \n",
       "2   3.8  [[Rated 1.0, RATED\\n  Cockroaches !! I Repeat ...  [Unknown]   \n",
       "3   3.7  [[Rated 1.5, RATED\\n  The food was not satisfa...  [Unknown]   \n",
       "4   3.8  [[Rated 4.0, RATED\\n  Great service, overwhelm...  [Unknown]   \n",
       "\n",
       "                           cuisines  \\\n",
       "0  [Chinese, Mughlai, North Indian]   \n",
       "1     [Chinese, North Indian, Thai]   \n",
       "2          [Cafe, Italian, Mexican]   \n",
       "3      [North Indian, South Indian]   \n",
       "4        [North Indian, Rajasthani]   \n",
       "\n",
       "                                          dish_liked  \n",
       "0  [Dum Biryani, Lunch Buffet, Masala Papad, Pane...  \n",
       "1  [Chicken Biryani, Chocolate Nirvana, Dum Birya...  \n",
       "2  [Cannelloni, Churros, Hot Chocolate, Minestron...  \n",
       "3                                      [Masala Dosa]  \n",
       "4                              [Gol Gappe, Panipuri]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. CORE LIBRARIES ---\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# In your main setup cell, replace the old NLTK section with this:\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. DATA HANDLING & ANALYSIS ---\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- 3. NATURAL LANGUAGE PROCESSING (NLP) ---\n",
    "from textblob import TextBlob  # For easy sentiment analysis\n",
    "\n",
    "# This will now work because you've manually downloaded the data.\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# --- 4. UTILITIES ---\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ===================================================================\n",
    "#                      CONFIGURATION\n",
    "# ===================================================================\n",
    "# (Your standard, excellent configuration settings)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# ... etc. ...\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    colorize=True,\n",
    "    format=(\n",
    "        \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n",
    "        \"<level>{level: <8}</level> | \"\n",
    "        \"<level>{message}</level>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "logger.info(\"✅ All libraries imported and configurations set successfully!\")\n",
    "\n",
    "# --- Load the NLP Dataset ---\n",
    "DATA_PATH = \"../data/processed/zomato_nlp.parquet\"\n",
    "try:\n",
    "    df_nlp = pd.read_parquet(DATA_PATH)\n",
    "    logger.success(f\"Successfully loaded the NLP dataset from '{DATA_PATH}'.\")\n",
    "    logger.info(f\"DataFrame shape: {df_nlp.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\n",
    "        f\"FATAL: The file was not found at '{DATA_PATH}'. Please ensure the path is correct.\"\n",
    "    )\n",
    "\n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1d327-a1ca-44c6-9afc-4a8b047c18a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Grand NLP Feature Engineering Plan\n",
    "\n",
    "This notebook is a comprehensive exploration of Natural Language Processing techniques, applied to the Zomato review dataset. Guided by the principles of structured experimentation, we will systematically build a rich set of features, progressing from fundamental text statistics to state-of-the-art deep learning models.\n",
    "\n",
    "Our workflow is divided into three major phases, inspired by the provided NLP mind map:\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 1: Foundational Text Analysis & Preprocessing**\n",
    "\n",
    "*Goal: To create a clean, standardized text corpus and extract basic, yet powerful, statistical features.*\n",
    "\n",
    "1.  **Text Aggregation & Cleaning:**\n",
    "    *   **Action:** Combine all English reviews for each restaurant into a single, unified text document.\n",
    "    *   **Action:** Create a \"god-level\" text preprocessing function that will lowercase text, remove punctuation, numbers, and URLs, and handle extra whitespace.\n",
    "2.  **Lexical Features (Readability & Complexity):**\n",
    "    *   **Action:** Engineer features based on the raw text, such as `total_review_length`, `avg_word_length`, and `readability_score` (e.g., Flesch-Kincaid). This tests the hypothesis that the *style* of reviews correlates with the rating.\n",
    "3.  **Tokenization & Stopword Removal:**\n",
    "    *   **Action:** Convert the cleaned text into a list of individual words (tokens).\n",
    "    *   **Action:** Remove common English stopwords (e.g., \"the\", \"a\", \"is\") to reduce noise and focus on meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 2: Classic NLP Feature Extraction**\n",
    "\n",
    "*Goal: To apply traditional, rule-based, and statistical NLP methods to extract sentiment and topic-based features.*\n",
    "\n",
    "1.  **Sentiment Analysis (The \"Vibe\" Score):**\n",
    "    *   **Action:** We will scientifically compare three different sentiment analysis techniques on the cleaned text:\n",
    "        1.  **TextBlob:** A simple, fast baseline.\n",
    "        2.  **VADER:** A lexicon and rule-based engine optimized for social media and review text.\n",
    "        3.  **Result:** Create `textblob_sentiment` and `vader_sentiment` features.\n",
    "2.  **Keyword & N-gram Analysis (The \"Bag-of-Words\" Approach):**\n",
    "    *   **Action:** We will use **TF-IDF (Term Frequency-Inverse Document Frequency)** on the tokenized text. This will convert our text into a numerical matrix where each column represents a word, and the value represents its importance to that restaurant's reviews.\n",
    "    *   **Feature Creation:** We will use dimensionality reduction techniques (like SVD or NMF) on the TF-IDF matrix to distill it into a few powerful, high-level \"topic\" features (e.g., `topic_food_quality`, `topic_service`, `topic_ambiance`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 3: State-of-the-Art Deep Learning (The Transformer Era)**\n",
    "\n",
    "*Goal: To leverage a pre-trained, multilingual deep learning model to capture the deepest contextual understanding of the text, including \"Hinglish\" and other nuances.*\n",
    "\n",
    "1.  **Contextual Sentiment Analysis:**\n",
    "    *   **Action:** We will use a powerful, pre-trained Transformer model (like `cardiffnlp/twitter-xlm-roberta-base-sentiment`) from the Hugging Face library.\n",
    "    *   **Key Advantage:** This model understands context, negation, and multilingual text far better than the classic methods. It will be run on the **original, unfiltered review text** to maximize its power.\n",
    "    *   **Result:** Create a `transformer_sentiment` feature, which will likely be our most powerful sentiment-based signal.\n",
    "\n",
    "By the end of this notebook, we will have engineered a wide array of NLP features, from simple counts to complex topic and sentiment scores. This will provide our final model with an incredibly rich understanding of the customer experience described in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf1c0a3-8e44-4e37-b3ea-50d06169b18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:45:26.898647Z",
     "iopub.status.busy": "2025-09-16T14:45:26.898423Z",
     "iopub.status.idle": "2025-09-16T14:45:36.002066Z",
     "shell.execute_reply": "2025-09-16T14:45:36.001433Z",
     "shell.execute_reply.started": "2025-09-16T14:45:26.898633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:15:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Starting Text Aggregation & Lexical Feature Engineering ---\u001b[0m\n",
      "\u001b[32m2025-09-16 20:15:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mAggregating all review texts for each restaurant...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ab960f97ef4a20aed357de48233d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:15:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mEngineering lexical features...\u001b[0m\n",
      "\u001b[32m2025-09-16 20:15:35\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAggregation and lexical feature creation complete.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>total_review_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>full_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>10</td>\n",
       "      <td>2906</td>\n",
       "      <td>4.662083</td>\n",
       "      <td>RATED\\n  Its a restaurant near to Banashankari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>14</td>\n",
       "      <td>4958</td>\n",
       "      <td>4.493304</td>\n",
       "      <td>RATED\\n  I had a very bad experience here.\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>20</td>\n",
       "      <td>6993</td>\n",
       "      <td>4.519108</td>\n",
       "      <td>RATED\\n  Cockroaches !! I Repeat cockroaches!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>23</td>\n",
       "      <td>7708</td>\n",
       "      <td>4.539797</td>\n",
       "      <td>RATED\\n  The food was not satisfactory. Not on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>2</td>\n",
       "      <td>651</td>\n",
       "      <td>5.252427</td>\n",
       "      <td>RATED\\n  Great service, overwhelming experienc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  review_count  total_review_length  avg_word_length  \\\n",
       "0                  Jalsa            10                 2906         4.662083   \n",
       "1         Spice Elephant            14                 4958         4.493304   \n",
       "2        San Churro Cafe            20                 6993         4.519108   \n",
       "3  Addhuri Udupi Bhojana            23                 7708         4.539797   \n",
       "4          Grand Village             2                  651         5.252427   \n",
       "\n",
       "                                    full_review_text  \n",
       "0  RATED\\n  Its a restaurant near to Banashankari...  \n",
       "1  RATED\\n  I had a very bad experience here.\\nI ...  \n",
       "2  RATED\\n  Cockroaches !! I Repeat cockroaches!!...  \n",
       "3  RATED\\n  The food was not satisfactory. Not on...  \n",
       "4  RATED\\n  Great service, overwhelming experienc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def aggregate_reviews_and_create_lexical_features(\n",
    "    df: pd.DataFrame, review_col: str = \"reviews_list\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates review texts into a single document per restaurant and\n",
    "    creates initial lexical (count-based) features.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Text Aggregation & Lexical Feature Engineering ---\")\n",
    "    df_out = df.copy()\n",
    "\n",
    "    def get_all_review_texts(review_array):\n",
    "        if len(review_array) > 0:\n",
    "            return [\n",
    "                review[1]\n",
    "                for review in review_array\n",
    "                if len(review) == 2 and isinstance(review[1], str)\n",
    "            ]\n",
    "        return []\n",
    "\n",
    "    logger.info(\"Aggregating all review texts for each restaurant...\")\n",
    "    df_out[\"full_review_text\"] = df_out[review_col].progress_apply(\n",
    "        lambda arr: \" \".join(get_all_review_texts(arr))\n",
    "    )\n",
    "\n",
    "    logger.info(\"Engineering lexical features...\")\n",
    "    df_out[\"review_count\"] = df_out[review_col].apply(len)\n",
    "    df_out[\"total_review_length\"] = df_out[\"full_review_text\"].str.len()\n",
    "\n",
    "    # Use a temporary series to avoid chained assignment warnings\n",
    "    avg_word_length_series = (\n",
    "        df_out[\"full_review_text\"]\n",
    "        .str.split()\n",
    "        .apply(\n",
    "            lambda tokens: np.mean([len(token) for token in tokens]) if tokens else 0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    # Assign the result of fillna back to the column instead of using inplace=True\n",
    "    df_out[\"avg_word_length\"] = avg_word_length_series.fillna(0)\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    logger.success(\"Aggregation and lexical feature creation complete.\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# --- Execute the first step ---\n",
    "df_nlp_features = aggregate_reviews_and_create_lexical_features(df_nlp)\n",
    "\n",
    "# --- Verification ---\n",
    "display(\n",
    "    df_nlp_features[\n",
    "        [\n",
    "            \"name\",\n",
    "            \"review_count\",\n",
    "            \"total_review_length\",\n",
    "            \"avg_word_length\",\n",
    "            \"full_review_text\",\n",
    "        ]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cb75c-acb4-48f5-a01d-7a31f83f111f",
   "metadata": {},
   "source": [
    "**Result:** The initial feature creation is a success. We have successfully aggregated all review texts into a new `full_review_text` column. Additionally, we have created three new numerical features: `review_count`, `total_review_length`, and `avg_word_length`. These will serve as our first set of NLP-derived predictors. The next step is to perform advanced preprocessing on the `full_review_text` to prepare it for sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c89759-291b-4e72-83f8-4f1b26295b4c",
   "metadata": {},
   "source": [
    "### 3.1. Advanced Text Preprocessing\n",
    "\n",
    "The `full_review_text` column contains raw, unstructured text. To prepare it for more advanced NLP tasks, we need to clean and standardize it. The following function creates a comprehensive preprocessing pipeline that will:\n",
    "\n",
    "1.  **Lowercase** all text for consistency.\n",
    "2.  Remove the recurring **\"RATED\\n\"** prefix.\n",
    "3.  Use regular expressions to **remove all punctuation, numbers, and special characters**, leaving only letters and spaces.\n",
    "4.  **Tokenize** the text by splitting it into individual words.\n",
    "5.  Remove common English **stopwords** (e.g., \"the\", \"a\", \"is\") which add little semantic value.\n",
    "6.  Perform **lemmatization**, which intelligently reduces words to their root form (e.g., \"running,\" \"ran,\" and \"runs\" all become \"run\"). This is more advanced than simple stemming.\n",
    "7.  Join the cleaned tokens back into a final, processed string.\n",
    "\n",
    "This will create a new `processed_text` column, which will be the ideal input for our subsequent NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9dfbf6-19b2-49e0-aebe-f73a0fe785dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:45:36.002811Z",
     "iopub.status.busy": "2025-09-16T14:45:36.002626Z",
     "iopub.status.idle": "2025-09-16T14:46:25.864833Z",
     "shell.execute_reply": "2025-09-16T14:46:25.864113Z",
     "shell.execute_reply.started": "2025-09-16T14:45:36.002796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Creating Specialized Text Corpora from 'full_review_text' ---\u001b[0m\n",
      "\u001b[32m2025-09-16 20:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCreating 'text_for_sentiment' (lightly cleaned)...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3d187305404803a5cec5a5e8d906f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:15:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCreating 'text_for_topics' (heavily cleaned and lemmatized)...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf32889d6b534a64a44d7c8c885e55a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:16:25\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSpecialized text corpora created successfully.\u001b[0m\n",
      "\n",
      "--- Verification of Specialized Text Columns ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>full_review_text</th>\n",
       "      <th>text_for_sentiment</th>\n",
       "      <th>text_for_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>RATED\\n  Its a restaurant near to Banashankari...</td>\n",
       "      <td>its a restaurant near to banashankari bda. me ...</td>\n",
       "      <td>restaurant near banashankari bda along office ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>RATED\\n  I had a very bad experience here.\\nI ...</td>\n",
       "      <td>i had a very bad experience here.\\ni don't kno...</td>\n",
       "      <td>bad experience know carte buffet worst gave co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>RATED\\n  Cockroaches !! I Repeat cockroaches!!...</td>\n",
       "      <td>cockroaches !! i repeat cockroaches!!bakasura ...</td>\n",
       "      <td>cockroach repeat cockroach bakasura disappoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>RATED\\n  The food was not satisfactory. Not on...</td>\n",
       "      <td>the food was not satisfactory. not one item se...</td>\n",
       "      <td>food satisfactory one item served could eaten ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>RATED\\n  Great service, overwhelming experienc...</td>\n",
       "      <td>great service, overwhelming experience.\\n\\none...</td>\n",
       "      <td>great service overwhelming experience one kind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                   full_review_text  \\\n",
       "0                  Jalsa  RATED\\n  Its a restaurant near to Banashankari...   \n",
       "1         Spice Elephant  RATED\\n  I had a very bad experience here.\\nI ...   \n",
       "2        San Churro Cafe  RATED\\n  Cockroaches !! I Repeat cockroaches!!...   \n",
       "3  Addhuri Udupi Bhojana  RATED\\n  The food was not satisfactory. Not on...   \n",
       "4          Grand Village  RATED\\n  Great service, overwhelming experienc...   \n",
       "\n",
       "                                  text_for_sentiment  \\\n",
       "0  its a restaurant near to banashankari bda. me ...   \n",
       "1  i had a very bad experience here.\\ni don't kno...   \n",
       "2  cockroaches !! i repeat cockroaches!!bakasura ...   \n",
       "3  the food was not satisfactory. not one item se...   \n",
       "4  great service, overwhelming experience.\\n\\none...   \n",
       "\n",
       "                                     text_for_topics  \n",
       "0  restaurant near banashankari bda along office ...  \n",
       "1  bad experience know carte buffet worst gave co...  \n",
       "2  cockroach repeat cockroach bakasura disappoint...  \n",
       "3  food satisfactory one item served could eaten ...  \n",
       "4  great service overwhelming experience one kind...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# (Assume NLTK data is already downloaded)\n",
    "\n",
    "\n",
    "def create_specialized_text_corpora(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates two specialized text columns for different NLP tasks:\n",
    "    1. A lightly cleaned version for sentiment analysis.\n",
    "    2. A heavily cleaned version for topic modeling.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Creating Specialized Text Corpora from '{text_col}' ---\")\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # --- Tools for Heavy Cleaning ---\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    non_alpha_pattern = re.compile(r\"[^a-z\\s]\")\n",
    "\n",
    "    # --- 1. Create 'text_for_sentiment' (Light Cleaning) ---\n",
    "    def light_clean(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Just lowercase and remove the \"RATED\" prefix. Keep everything else!\n",
    "        text = text.lower().replace(\"rated\\n\", \" \").strip()\n",
    "        return text\n",
    "\n",
    "    logger.info(\"Creating 'text_for_sentiment' (lightly cleaned)...\")\n",
    "    df_out[\"text_for_sentiment\"] = df_out[text_col].progress_apply(light_clean)\n",
    "\n",
    "    # --- 2. Create 'text_for_topics' (Heavy Cleaning) ---\n",
    "    def heavy_clean(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower().replace(\"rated\\n\", \" \").strip()\n",
    "        text = non_alpha_pattern.sub(\" \", text)  # Remove punctuation, numbers, emojis\n",
    "        tokens = text.split()\n",
    "        lemmatized_tokens = [\n",
    "            lemmatizer.lemmatize(word)\n",
    "            for word in tokens\n",
    "            if word not in stop_words and len(word) > 2\n",
    "        ]\n",
    "        return \" \".join(lemmatized_tokens)\n",
    "\n",
    "    logger.info(\"Creating 'text_for_topics' (heavily cleaned and lemmatized)...\")\n",
    "    df_out[\"text_for_topics\"] = df_out[text_col].progress_apply(heavy_clean)\n",
    "\n",
    "    logger.success(\"Specialized text corpora created successfully.\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# --- Execute Preprocessing ---\n",
    "# df_nlp_features is the output from our first step (aggregation)\n",
    "df_nlp_processed = create_specialized_text_corpora(\n",
    "    df_nlp_features, text_col=\"full_review_text\"\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification of Specialized Text Columns ---\")\n",
    "display(\n",
    "    df_nlp_processed[\n",
    "        [\"name\", \"full_review_text\", \"text_for_sentiment\", \"text_for_topics\"]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a8ba7-943f-4f35-a0e0-136e229cf6cf",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Classic NLP Feature Extraction\n",
    "\n",
    "With a clean, preprocessed text corpus, we can now move on to extracting meaningful features. This phase focuses on applying classic, well-established NLP techniques to quantify the subjective aspects of the reviews.\n",
    "\n",
    "### 4.1. Sentiment Analysis: Quantifying the \"Vibe\"\n",
    "\n",
    "Our first and most important task is to quantify the sentiment of the reviews. A numerical sentiment score has the potential to be a very strong predictor of the restaurant's `rate`.\n",
    "\n",
    "We will implement a function that calculates sentiment using two different popular libraries, allowing us to compare their results:\n",
    "\n",
    "1.  **TextBlob:** A simple and fast library that provides a **polarity score** ranging from -1 (very negative) to +1 (very positive). It's a great baseline.\n",
    "2.  **VADER (Valence Aware Dictionary and sEntiment Reasoner):** A more advanced, rule-based sentiment analysis tool specifically tuned for social media and review text. It's better at handling negation (e.g., \"not good\"), emphasis (e.g., \"SOOO GOOD!!\"), and slang. It provides positive, negative, neutral, and a final combined **compound score** (also from -1 to +1).\n",
    "\n",
    "This function will take our `processed_text` and create new numerical features for each sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde94376-ab18-4b7f-b276-b433fd295b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:46:25.865658Z",
     "iopub.status.busy": "2025-09-16T14:46:25.865436Z",
     "iopub.status.idle": "2025-09-16T15:24:30.130808Z",
     "shell.execute_reply": "2025-09-16T15:24:30.127161Z",
     "shell.execute_reply.started": "2025-09-16T14:46:25.865641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:16:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Generating Sentiment Features for 'text_for_sentiment' ---\u001b[0m\n",
      "\u001b[32m2025-09-16 20:16:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating TextBlob polarity scores...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6b0ddc0f2d4a6b93a5288b9176adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:18:04\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mTextBlob sentiment calculated.\u001b[0m\n",
      "\u001b[32m2025-09-16 20:18:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mCalculating VADER compound scores...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe61a7bc15a4750a803a154b98ceeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:54:30\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mVADER sentiment calculated.\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:30\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAll sentiment features created successfully.\u001b[0m\n",
      "\n",
      "--- Verification of Sentiment Features ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rate</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.345060</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.195731</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.166162</td>\n",
       "      <td>0.9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.309284</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.447883</td>\n",
       "      <td>0.9856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  rate  sentiment_textblob  sentiment_vader\n",
       "0                  Jalsa   4.1            0.345060           0.9996\n",
       "1         Spice Elephant   4.1            0.195731           0.9996\n",
       "2        San Churro Cafe   3.8            0.166162           0.9997\n",
       "3  Addhuri Udupi Bhojana   3.7            0.309284           0.9998\n",
       "4          Grand Village   3.8            0.447883           0.9856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from textblob import TextBlob\n",
    "from tqdm.auto import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def generate_sentiment_features(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates sentiment scores using both TextBlob and VADER.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Generating Sentiment Features for '{text_col}' ---\")\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # --- Initialize VADER once for efficiency ---\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # --- TextBlob Polarity ---\n",
    "    logger.info(\"Calculating TextBlob polarity scores...\")\n",
    "    df_out[\"sentiment_textblob\"] = df_out[text_col].progress_apply(\n",
    "        lambda text: TextBlob(text).sentiment.polarity\n",
    "    )\n",
    "    logger.success(\"TextBlob sentiment calculated.\")\n",
    "\n",
    "    # --- VADER Compound Score ---\n",
    "    logger.info(\"Calculating VADER compound scores...\")\n",
    "    df_out[\"sentiment_vader\"] = df_out[text_col].progress_apply(\n",
    "        lambda text: vader_analyzer.polarity_scores(text)[\"compound\"]\n",
    "    )\n",
    "    logger.success(\"VADER sentiment calculated.\")\n",
    "\n",
    "    logger.success(\"All sentiment features created successfully.\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# --- NOW, run sentiment analysis on the CORRECT column ---\n",
    "# (The sentiment analysis function code remains the same)\n",
    "df_nlp_sentiments = generate_sentiment_features(\n",
    "    df_nlp_processed, text_col=\"text_for_sentiment\"\n",
    ")\n",
    "\n",
    "# --- Verification of Sentiment ---\n",
    "print(\"\\n--- Verification of Sentiment Features ---\")\n",
    "display(\n",
    "    df_nlp_sentiments[[\"name\", \"rate\", \"sentiment_textblob\", \"sentiment_vader\"]].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34121a-5ebf-4527-a344-cddaab7d6813",
   "metadata": {},
   "source": [
    "### 4.2. Keyword & Topic Analysis with TF-IDF\n",
    "\n",
    "While sentiment scores tell us if reviews are positive or negative, they don't tell us *why*. Are customers happy about the `food`, the `ambiance`, or the `service`? To answer this, we will use a powerful technique called **TF-IDF**.\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It identifies words that are:\n",
    "1.  **Frequent** within a single restaurant's reviews (high Term Frequency).\n",
    "2.  **Rare** across all other restaurants' reviews (high Inverse Document Frequency).\n",
    "\n",
    "This allows us to automatically discover the most important and characteristic keywords for each restaurant. For example, the word \"biryani\" might be common everywhere, but \"wood-fired\" might be a highly important, characteristic term for a specific set of pizza places.\n",
    "\n",
    "Our plan is to:\n",
    "1.  Apply a `TfidfVectorizer` to our `processed_text` corpus.\n",
    "2.  This will create a massive matrix where rows are restaurants and columns are words.\n",
    "3.  We will then use this matrix to understand the key topics in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a3f1f1-f26a-428a-870f-580598ee108a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:30.134503Z",
     "iopub.status.busy": "2025-09-16T15:24:30.134097Z",
     "iopub.status.idle": "2025-09-16T15:24:51.614466Z",
     "shell.execute_reply": "2025-09-16T15:24:51.613746Z",
     "shell.execute_reply.started": "2025-09-16T15:24:30.134473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:54:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Building TF-IDF Matrix from 'text_for_topics' ---\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFitting the TfidfVectorizer to the corpus...\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:51\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mTF-IDF matrix built successfully.\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mMatrix Shape: (45187, 3000) (Restaurants x Features/Terms)\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mDisplaying a sample of the 20 most important features (terms) learned:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>food</td>\n",
       "      <td>4557.806792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>good</td>\n",
       "      <td>4293.974173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>place</td>\n",
       "      <td>4244.043175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>chicken</td>\n",
       "      <td>2740.858407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>service</td>\n",
       "      <td>2006.092024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>taste</td>\n",
       "      <td>2000.145988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>ordered</td>\n",
       "      <td>1890.284472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>biryani</td>\n",
       "      <td>1625.688771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ambience</td>\n",
       "      <td>1604.463496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>time</td>\n",
       "      <td>1514.436362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>really</td>\n",
       "      <td>1461.502862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>great</td>\n",
       "      <td>1456.474888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>try</td>\n",
       "      <td>1438.121114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>1387.846994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>nice</td>\n",
       "      <td>1297.411958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>like</td>\n",
       "      <td>1284.987827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>order</td>\n",
       "      <td>1250.192224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>best</td>\n",
       "      <td>1179.793631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>pizza</td>\n",
       "      <td>1104.135277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>staff</td>\n",
       "      <td>1099.921725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term        score\n",
       "909         food  4557.806792\n",
       "1069        good  4293.974173\n",
       "1951       place  4244.043175\n",
       "403      chicken  2740.858407\n",
       "2386     service  2006.092024\n",
       "2648       taste  2000.145988\n",
       "1792     ordered  1890.284472\n",
       "237      biryani  1625.688771\n",
       "60      ambience  1604.463496\n",
       "2718        time  1514.436362\n",
       "2196      really  1461.502862\n",
       "1148       great  1456.474888\n",
       "2777         try  1438.121114\n",
       "2258  restaurant  1387.846994\n",
       "1711        nice  1297.411958\n",
       "1444        like  1284.987827\n",
       "1784       order  1250.192224\n",
       "210         best  1179.793631\n",
       "1945       pizza  1104.135277\n",
       "2534       staff  1099.921725"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np  # Make sure numpy is imported\n",
    "from loguru import logger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def build_tfidf_matrix(df: pd.DataFrame, text_col: str):\n",
    "    \"\"\"\n",
    "    Builds a TF-IDF matrix from a text column and returns the vectorizer\n",
    "    and the resulting matrix.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Building TF-IDF Matrix from '{text_col}' ---\")\n",
    "\n",
    "    if text_col not in df.columns:\n",
    "        logger.error(\n",
    "            f\"FATAL: Column '{text_col}' not found in the DataFrame. Cannot build TF-IDF matrix.\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=5,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        stop_words=\"english\",  # More robust than our manual list for this tool\n",
    "    )\n",
    "\n",
    "    logger.info(\"Fitting the TfidfVectorizer to the corpus...\")\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_col])\n",
    "\n",
    "    logger.success(\"TF-IDF matrix built successfully.\")\n",
    "    logger.info(f\"Matrix Shape: {tfidf_matrix.shape} (Restaurants x Features/Terms)\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Displaying a sample of the 20 most important features (terms) learned:\"\n",
    "    )\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
    "    df_tfidf_scores = pd.DataFrame({\"term\": feature_names, \"score\": tfidf_scores})\n",
    "    display(df_tfidf_scores.sort_values(by=\"score\", ascending=False).head(20))\n",
    "\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "# --- Execute TF-IDF (with the CORRECT column name) ---\n",
    "\n",
    "# <<--- THE FIX IS HERE --- >>\n",
    "# We must use the 'text_for_topics' column, which was specifically created for this task.\n",
    "tfidf_vectorizer, tfidf_matrix = build_tfidf_matrix(\n",
    "    df_nlp_sentiments, text_col=\"text_for_topics\"\n",
    ")\n",
    "# --- END OF FIX ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ba07c-a347-444a-94c1-dcd53ebbbd8d",
   "metadata": {},
   "source": [
    "### 4.3. Topic Modeling with Latent Semantic Analysis (LSA)\n",
    "\n",
    "We have successfully created a TF-IDF matrix, which represents the importance of 3,000 key terms for each restaurant. However, adding 3,000 new features to our model is computationally impractical.\n",
    "\n",
    "Our next step is to use **Dimensionality Reduction** to distill this vast matrix into a small number of high-level **\"topics\"**. We will use a technique called **Latent Semantic Analysis (LSA)**, which is implemented using **Truncated SVD (Singular Value Decomposition)**.\n",
    "\n",
    "LSA will analyze the co-occurrence patterns of words in our TF-IDF matrix and automatically group them into a specified number of topics. For each restaurant, we will then get a score for each of these topics, effectively creating powerful new features like `topic_food_quality_score` or `topic_ambiance_service_score`. This allows us to capture the core themes of the reviews in a very compact and model-friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c0a61a-992c-4d2f-9acc-3d7d75b953b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:51.617545Z",
     "iopub.status.busy": "2025-09-16T15:24:51.617322Z",
     "iopub.status.idle": "2025-09-16T15:24:53.213277Z",
     "shell.execute_reply": "2025-09-16T15:24:53.212433Z",
     "shell.execute_reply.started": "2025-09-16T15:24:51.617528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 20:54:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Extracting 10 Topics using LSA (TruncatedSVD) ---\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:52\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mLSA model fitted and data transformed successfully.\u001b[0m\n",
      "\u001b[32m2025-09-16 20:54:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Top 10 Terms per Discovered Topic ---\u001b[0m\n",
      "Topic 0: food, good, place, chicken, service, ambience, taste, ordered, biryani, great\n",
      "Topic 1: biryani, chicken, biriyani, rice, food, mutton, delivery, restaurant, chicken biryani, taste\n",
      "Topic 2: cake, cream, ice cream, ice, chocolate, biryani, waffle, taste, order, ordered\n",
      "Topic 3: pizza, chicken, biryani, burger, beer, pasta, biriyani, drink, ambience, music\n",
      "Topic 4: cake, pizza, pastry, order, birthday, cupcake, bakery, delivery, ordered, burger\n",
      "Topic 5: pizza, burger, delivery, order, ordered, sandwich, cheese, pasta, shake, taste\n",
      "Topic 6: biryani, dosa, coffee, place, breakfast, tea, masala dosa, biriyani, cafe, south\n",
      "Topic 7: pizza, biryani, buffet, dosa, indian, ice cream, cream, crust, ice, south\n",
      "Topic 8: chicken, dosa, pizza, fish, noodle, buffet, rice, chinese, fried, roll\n",
      "Topic 9: burger, food, dosa, fry, fish, kerala, beer, cake, ice cream, ice\n",
      "\u001b[32m2025-09-16 20:54:53\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mSuccessfully merged new topic features into the main NLP DataFrame.\u001b[0m\n",
      "\n",
      "--- Verification of New Topic Features ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rate</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>-0.053563</td>\n",
       "      <td>-0.142821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.602828</td>\n",
       "      <td>-0.031769</td>\n",
       "      <td>-0.063036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.331411</td>\n",
       "      <td>-0.179015</td>\n",
       "      <td>0.001051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.541989</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>-0.120429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>-0.022821</td>\n",
       "      <td>-0.090174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  rate   topic_0   topic_1   topic_2\n",
       "0                  Jalsa   4.1  0.482831 -0.053563 -0.142821\n",
       "1         Spice Elephant   4.1  0.602828 -0.031769 -0.063036\n",
       "2        San Churro Cafe   3.8  0.331411 -0.179015  0.001051\n",
       "3  Addhuri Udupi Bhojana   3.7  0.541989  0.015496 -0.120429\n",
       "4          Grand Village   3.8  0.220451 -0.022821 -0.090174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def extract_topics_with_lsa(tfidf_matrix, tfidf_vectorizer, n_topics: int = 10):\n",
    "    \"\"\"\n",
    "    Applies Latent Semantic Analysis (LSA) via TruncatedSVD to a TF-IDF matrix\n",
    "    to discover latent topics.\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix: The sparse matrix from the TfidfVectorizer.\n",
    "        tfidf_vectorizer: The fitted TfidfVectorizer instance.\n",
    "        n_topics (int): The number of topics to extract.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where rows are restaurants and columns are topic scores.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Extracting {n_topics} Topics using LSA (TruncatedSVD) ---\")\n",
    "\n",
    "    # --- 1. Apply TruncatedSVD ---\n",
    "    # We use SVD to reduce the dimensionality of our 3000-column matrix\n",
    "    lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    topic_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "    logger.success(\"LSA model fitted and data transformed successfully.\")\n",
    "\n",
    "    # --- 2. Analyze the Topics ---\n",
    "    # Let's inspect what words define each topic\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    logger.info(\"--- Top 10 Terms per Discovered Topic ---\")\n",
    "    for i, comp in enumerate(lsa.components_):\n",
    "        # Sort the terms by their weight in the current topic\n",
    "        terms_in_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_in_comp, key=lambda x: x[1], reverse=True)[:10]\n",
    "        top_terms = [t[0] for t in sorted_terms]\n",
    "        print(f\"Topic {i}: {', '.join(top_terms)}\")\n",
    "\n",
    "    # --- 3. Create the Final DataFrame ---\n",
    "    # Create column names for our new features\n",
    "    topic_col_names = [f\"topic_{i}\" for i in range(n_topics)]\n",
    "    df_topics = pd.DataFrame(topic_matrix, columns=topic_col_names)\n",
    "\n",
    "    return df_topics\n",
    "\n",
    "\n",
    "# --- Execute LSA ---\n",
    "# tfidf_matrix and tfidf_vectorizer are from our previous step\n",
    "# Let's extract 10 topics as a starting point\n",
    "df_topic_features = extract_topics_with_lsa(tfidf_matrix, tfidf_vectorizer, n_topics=10)\n",
    "\n",
    "# --- Merge the new topic features back into our main NLP dataframe ---\n",
    "# First, we need to reset the index of our main df to ensure a clean merge\n",
    "df_nlp_final = df_nlp_sentiments.reset_index(drop=True)\n",
    "df_nlp_final = pd.concat([df_nlp_final, df_topic_features], axis=1)\n",
    "\n",
    "logger.success(\"Successfully merged new topic features into the main NLP DataFrame.\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification of New Topic Features ---\")\n",
    "display(df_nlp_final[[\"name\", \"rate\", \"topic_0\", \"topic_1\", \"topic_2\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a311b4fa-282e-4649-98b2-c851c51ae66f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.214030Z",
     "iopub.status.busy": "2025-09-16T15:24:53.213792Z",
     "iopub.status.idle": "2025-09-16T15:24:53.260790Z",
     "shell.execute_reply": "2025-09-16T15:24:53.259989Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.214015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   name          45187 non-null  object \n",
      " 1   address       45187 non-null  object \n",
      " 2   rate          45187 non-null  float64\n",
      " 3   reviews_list  45187 non-null  object \n",
      " 4   menu_item     45187 non-null  object \n",
      " 5   cuisines      45187 non-null  object \n",
      " 6   dish_liked    45187 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472ec176-f2ac-4568-8334-dde2dfbb325c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.261538Z",
     "iopub.status.busy": "2025-09-16T15:24:53.261357Z",
     "iopub.status.idle": "2025-09-16T15:24:53.294956Z",
     "shell.execute_reply": "2025-09-16T15:24:53.294208Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.261523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   name                 45187 non-null  object \n",
      " 1   address              45187 non-null  object \n",
      " 2   rate                 45187 non-null  float64\n",
      " 3   reviews_list         45187 non-null  object \n",
      " 4   menu_item            45187 non-null  object \n",
      " 5   cuisines             45187 non-null  object \n",
      " 6   dish_liked           45187 non-null  object \n",
      " 7   full_review_text     45187 non-null  object \n",
      " 8   review_count         45187 non-null  int64  \n",
      " 9   total_review_length  45187 non-null  int64  \n",
      " 10  avg_word_length      45187 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad6e937-d837-4476-bd64-60922650b977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.295837Z",
     "iopub.status.busy": "2025-09-16T15:24:53.295635Z",
     "iopub.status.idle": "2025-09-16T15:24:53.324509Z",
     "shell.execute_reply": "2025-09-16T15:24:53.323339Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.295821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 25 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   name                 45187 non-null  object \n",
      " 1   address              45187 non-null  object \n",
      " 2   rate                 45187 non-null  float64\n",
      " 3   reviews_list         45187 non-null  object \n",
      " 4   menu_item            45187 non-null  object \n",
      " 5   cuisines             45187 non-null  object \n",
      " 6   dish_liked           45187 non-null  object \n",
      " 7   full_review_text     45187 non-null  object \n",
      " 8   review_count         45187 non-null  int64  \n",
      " 9   total_review_length  45187 non-null  int64  \n",
      " 10  avg_word_length      45187 non-null  float64\n",
      " 11  text_for_sentiment   45187 non-null  object \n",
      " 12  text_for_topics      45187 non-null  object \n",
      " 13  sentiment_textblob   45187 non-null  float64\n",
      " 14  sentiment_vader      45187 non-null  float64\n",
      " 15  topic_0              45187 non-null  float64\n",
      " 16  topic_1              45187 non-null  float64\n",
      " 17  topic_2              45187 non-null  float64\n",
      " 18  topic_3              45187 non-null  float64\n",
      " 19  topic_4              45187 non-null  float64\n",
      " 20  topic_5              45187 non-null  float64\n",
      " 21  topic_6              45187 non-null  float64\n",
      " 22  topic_7              45187 non-null  float64\n",
      " 23  topic_8              45187 non-null  float64\n",
      " 24  topic_9              45187 non-null  float64\n",
      "dtypes: float64(14), int64(2), object(9)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0229217-ffc7-425c-b587-d833a33933d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.326107Z",
     "iopub.status.busy": "2025-09-16T15:24:53.325697Z",
     "iopub.status.idle": "2025-09-16T15:24:53.376798Z",
     "shell.execute_reply": "2025-09-16T15:24:53.376083Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.326081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   name                 45187 non-null  object \n",
      " 1   address              45187 non-null  object \n",
      " 2   rate                 45187 non-null  float64\n",
      " 3   reviews_list         45187 non-null  object \n",
      " 4   menu_item            45187 non-null  object \n",
      " 5   cuisines             45187 non-null  object \n",
      " 6   dish_liked           45187 non-null  object \n",
      " 7   full_review_text     45187 non-null  object \n",
      " 8   review_count         45187 non-null  int64  \n",
      " 9   total_review_length  45187 non-null  int64  \n",
      " 10  avg_word_length      45187 non-null  float64\n",
      " 11  text_for_sentiment   45187 non-null  object \n",
      " 12  text_for_topics      45187 non-null  object \n",
      "dtypes: float64(2), int64(2), object(9)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42ab3be-6b81-4d5c-991c-ef55d9b2a46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.378116Z",
     "iopub.status.busy": "2025-09-16T15:24:53.377908Z",
     "iopub.status.idle": "2025-09-16T15:24:53.403750Z",
     "shell.execute_reply": "2025-09-16T15:24:53.402941Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.378100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   name                 45187 non-null  object \n",
      " 1   address              45187 non-null  object \n",
      " 2   rate                 45187 non-null  float64\n",
      " 3   reviews_list         45187 non-null  object \n",
      " 4   menu_item            45187 non-null  object \n",
      " 5   cuisines             45187 non-null  object \n",
      " 6   dish_liked           45187 non-null  object \n",
      " 7   full_review_text     45187 non-null  object \n",
      " 8   review_count         45187 non-null  int64  \n",
      " 9   total_review_length  45187 non-null  int64  \n",
      " 10  avg_word_length      45187 non-null  float64\n",
      " 11  text_for_sentiment   45187 non-null  object \n",
      " 12  text_for_topics      45187 non-null  object \n",
      " 13  sentiment_textblob   45187 non-null  float64\n",
      " 14  sentiment_vader      45187 non-null  float64\n",
      "dtypes: float64(4), int64(2), object(9)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp_sentiments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a6a9a9-7d69-406b-89f4-dea49b896f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:24:53.404824Z",
     "iopub.status.busy": "2025-09-16T15:24:53.404546Z",
     "iopub.status.idle": "2025-09-16T15:24:53.416586Z",
     "shell.execute_reply": "2025-09-16T15:24:53.415626Z",
     "shell.execute_reply.started": "2025-09-16T15:24:53.404803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   topic_0  45187 non-null  float64\n",
      " 1   topic_1  45187 non-null  float64\n",
      " 2   topic_2  45187 non-null  float64\n",
      " 3   topic_3  45187 non-null  float64\n",
      " 4   topic_4  45187 non-null  float64\n",
      " 5   topic_5  45187 non-null  float64\n",
      " 6   topic_6  45187 non-null  float64\n",
      " 7   topic_7  45187 non-null  float64\n",
      " 8   topic_8  45187 non-null  float64\n",
      " 9   topic_9  45187 non-null  float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 3.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_topic_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eab54e-55aa-4dc8-b7d0-2a92752889b6",
   "metadata": {},
   "source": [
    "## 5. Phase 3: State-of-the-Art Deep Learning with Transformers\n",
    "\n",
    "Having extracted features using classic NLP techniques, we now advance to the cutting edge. Our dataset contains a mix of languages (\"Hinglish\") and informal text that can confuse simpler models like TextBlob and VADER.\n",
    "\n",
    "To overcome this, we will deploy a **pre-trained, multilingual Transformer model**. Specifically, we will use `cardiffnlp/twitter-xlm-roberta-base-sentiment`, a powerful model from the Hugging Face Hub that is fine-tuned on multilingual social media text.\n",
    "\n",
    "**Key Advantages of this Approach:**\n",
    "1.  **Contextual Understanding:** Unlike bag-of-words models, Transformers understand the order of words and the context in which they appear. It can differentiate between \"good food\" and \"not good food.\"\n",
    "2.  **Multilingual & Code-Mixed Capability:** This model was trained on diverse, real-world text and can effectively process the \"Hinglish\" and other languages present in our reviews.\n",
    "3.  **No Data Filtering:** We can run this model on our **original, unfiltered review text**, ensuring we extract a signal from 100% of our available data, maximizing its value.\n",
    "\n",
    "This process will be computationally intensive, but it will yield our premium sentiment feature: `transformer_sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f008823-1bfc-4b19-a3f8-659d81a5e767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T05:42:03.021125Z",
     "iopub.status.busy": "2025-09-17T05:42:03.020880Z",
     "iopub.status.idle": "2025-09-17T06:46:56.880806Z",
     "shell.execute_reply": "2025-09-17T06:46:56.871052Z",
     "shell.execute_reply.started": "2025-09-17T05:42:03.021110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 11:12:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- 3. Generating State-of-the-Art Transformer Sentiment ---\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/tools/miniconda3/envs/newAge/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 11:12:12\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mMultilingual sentiment model initialized successfully ON GPU.\u001b[0m\n",
      "\u001b[32m2025-09-17 11:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mApplying Transformer sentiment analysis... (This is the long one, go grab a coffee)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1f9fbe444f42a8be8d5b4883abac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 12:16:43\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mTransformer sentiment analysis complete.\u001b[0m\n",
      "\u001b[32m2025-09-17 12:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Final NLP Feature Set ---\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rate</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>transformer_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>0.606136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.602828</td>\n",
       "      <td>0.409859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.331411</td>\n",
       "      <td>0.188120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.541989</td>\n",
       "      <td>0.185093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>0.668551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  rate  sentiment_vader   topic_0  \\\n",
       "0                  Jalsa   4.1           0.9996  0.482831   \n",
       "1         Spice Elephant   4.1           0.9996  0.602828   \n",
       "2        San Churro Cafe   3.8           0.9997  0.331411   \n",
       "3  Addhuri Udupi Bhojana   3.7           0.9998  0.541989   \n",
       "4          Grand Village   3.8           0.9856  0.220451   \n",
       "\n",
       "   transformer_sentiment  \n",
       "0               0.606136  \n",
       "1               0.409859  \n",
       "2               0.188120  \n",
       "3               0.185093  \n",
       "4               0.668551  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 12:16:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mExporting final NLP features to '../data/processed/zomato_nlp_features_final.parquet'...\u001b[0m\n",
      "\u001b[32m2025-09-17 12:16:56\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mFinal NLP features saved successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def generate_transformer_sentiment(\n",
    "    df: pd.DataFrame, review_col: str = \"reviews_list\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The final, robust version for calculating Transformer sentiment using the online model.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- 3. Generating State-of-the-Art Transformer Sentiment ---\")\n",
    "    df_out = df.copy()\n",
    "\n",
    "    try:\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=0,  # Use GPU\n",
    "        )\n",
    "        logger.success(\"Multilingual sentiment model initialized successfully ON GPU.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Failed to initialize on GPU. Error: {e}. Falling back to CPU (will be slow).\"\n",
    "        )\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=-1,\n",
    "        )\n",
    "\n",
    "    def get_avg_sentiment(review_array):\n",
    "        if not hasattr(review_array, \"__len__\") or len(review_array) == 0:\n",
    "            return 0\n",
    "        review_texts = [\n",
    "            review[1]\n",
    "            for review in review_array\n",
    "            if len(review) == 2 and isinstance(review[1], str)\n",
    "        ]\n",
    "        if not review_texts:\n",
    "            return 0\n",
    "        sentiments = []\n",
    "        try:\n",
    "            # Added batch_size for better GPU performance. Removed trust_remote_code.\n",
    "            results = sentiment_pipeline(\n",
    "                review_texts, truncation=True, max_length=512, batch_size=16\n",
    "            )\n",
    "            for result in results:\n",
    "                if result[\"label\"] == \"positive\":\n",
    "                    sentiments.append(1 * result[\"score\"])\n",
    "                elif result[\"label\"] == \"negative\":\n",
    "                    sentiments.append(-1 * result[\"score\"])\n",
    "                else:\n",
    "                    sentiments.append(0.0)\n",
    "        except Exception:\n",
    "            # On error, we don't want to log thousands of times. Silently return neutral.\n",
    "            return 0\n",
    "        return np.mean(sentiments) if sentiments else 0\n",
    "\n",
    "    logger.info(\n",
    "        \"Applying Transformer sentiment analysis... (This is the long one, go grab a coffee)\"\n",
    "    )\n",
    "    # We run this on the original 'reviews_list' for maximum accuracy\n",
    "    df_out[\"transformer_sentiment\"] = df_out[review_col].progress_apply(\n",
    "        get_avg_sentiment\n",
    "    )\n",
    "\n",
    "    logger.success(\"Transformer sentiment analysis complete.\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# --- Execute ---\n",
    "# This is the final step, we use df_nlp_topics as the input\n",
    "df_nlp_final = generate_transformer_sentiment(df_nlp_final)\n",
    "\n",
    "# --- Verification & Export ---\n",
    "logger.info(\"--- Final NLP Feature Set ---\")\n",
    "display(\n",
    "    df_nlp_final[\n",
    "        [\"name\", \"rate\", \"sentiment_vader\", \"topic_0\", \"transformer_sentiment\"]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f02ed433-a329-4a66-81eb-7cddd9863b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T07:00:36.997142Z",
     "iopub.status.busy": "2025-09-17T07:00:36.995446Z",
     "iopub.status.idle": "2025-09-17T07:00:37.295350Z",
     "shell.execute_reply": "2025-09-17T07:00:37.294631Z",
     "shell.execute_reply.started": "2025-09-17T07:00:36.997088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45187 entries, 0 to 45186\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   name                   45187 non-null  object \n",
      " 1   address                45187 non-null  object \n",
      " 2   rate                   45187 non-null  float64\n",
      " 3   reviews_list           45187 non-null  object \n",
      " 4   menu_item              45187 non-null  object \n",
      " 5   cuisines               45187 non-null  object \n",
      " 6   dish_liked             45187 non-null  object \n",
      " 7   full_review_text       45187 non-null  object \n",
      " 8   review_count           45187 non-null  int64  \n",
      " 9   total_review_length    45187 non-null  int64  \n",
      " 10  avg_word_length        45187 non-null  float64\n",
      " 11  text_for_sentiment     45187 non-null  object \n",
      " 12  text_for_topics        45187 non-null  object \n",
      " 13  sentiment_textblob     45187 non-null  float64\n",
      " 14  sentiment_vader        45187 non-null  float64\n",
      " 15  topic_0                45187 non-null  float64\n",
      " 16  topic_1                45187 non-null  float64\n",
      " 17  topic_2                45187 non-null  float64\n",
      " 18  topic_3                45187 non-null  float64\n",
      " 19  topic_4                45187 non-null  float64\n",
      " 20  topic_5                45187 non-null  float64\n",
      " 21  topic_6                45187 non-null  float64\n",
      " 22  topic_7                45187 non-null  float64\n",
      " 23  topic_8                45187 non-null  float64\n",
      " 24  topic_9                45187 non-null  float64\n",
      " 25  transformer_sentiment  45187 non-null  float64\n",
      "dtypes: float64(15), int64(2), object(9)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nlp_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b15d3-52bf-4004-ad6c-642fbad18639",
   "metadata": {},
   "source": [
    "## 6. Phase 4: Feature Engineering with Word Embeddings\n",
    "\n",
    "To handle the high-cardinality, multi-value text columns (`menu_item`, `cuisines`, `dish_liked`), a simple one-hot encoding approach would be impractical. Instead, we have employed a state-of-the-art **representation learning** technique.\n",
    "\n",
    "### 6.1. Custom Word2Vec Model\n",
    "\n",
    "We implemented a \"titan-level\" pipeline to:\n",
    "1.  **Build a Unified Corpus:** All items from the `menu_item`, `cuisines`, and `dish_liked` columns were combined into a single, massive corpus.\n",
    "2.  **Train a Custom Word2Vec Model:** A `gensim.Word2Vec` model was trained on this corpus. This allows the model to learn the semantic relationships and context between food items specific to the Bangalore restaurant scene (e.g., learning that \"Biryani\" is similar to \"Mughlai\").\n",
    "3.  **Vectorize Restaurant Profiles:** For each restaurant, we calculated the average vector of all the items in its `menu_item`, `cuisines`, and `dish_liked` lists.\n",
    "\n",
    "**Result:**\n",
    "This process has successfully transformed our three most complex text columns into **60 new, dense, and powerful numerical features** (20 vector dimensions for each column). These embedding features provide our model with a deep, nuanced understanding of a restaurant's culinary profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d751d657-b9f5-4e31-ad7f-762417215bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:34:07.707701Z",
     "iopub.status.busy": "2025-09-17T13:34:07.706432Z",
     "iopub.status.idle": "2025-09-17T13:34:12.539540Z",
     "shell.execute_reply": "2025-09-17T13:34:12.538173Z",
     "shell.execute_reply.started": "2025-09-17T13:34:07.707658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 19:04:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Starting Simplified Feature Engineering with Custom Food Embeddings (v2) ---\u001b[0m\n",
      "\u001b[32m2025-09-17 19:04:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 1: Building a unified corpus...\u001b[0m\n",
      "\u001b[32m2025-09-17 19:04:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mUnified corpus created with 34,894 total documents.\u001b[0m\n",
      "\u001b[32m2025-09-17 19:04:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 2: Training a custom Word2Vec model...\u001b[0m\n",
      "\u001b[32m2025-09-17 19:04:09\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mCustom Word2Vec model trained successfully.\u001b[0m\n",
      "\u001b[32m2025-09-17 19:04:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStep 3: Vectorizing each column...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267c0a6291d941cd84ec9c26aa34baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Embedding Features:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 19:04:12\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mAll specified columns have been converted into embedding features.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>rate</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>full_review_text</th>\n",
       "      <th>review_count</th>\n",
       "      <th>total_review_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>text_for_sentiment</th>\n",
       "      <th>text_for_topics</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>transformer_sentiment</th>\n",
       "      <th>menu_item_vec_0</th>\n",
       "      <th>menu_item_vec_1</th>\n",
       "      <th>menu_item_vec_2</th>\n",
       "      <th>menu_item_vec_3</th>\n",
       "      <th>menu_item_vec_4</th>\n",
       "      <th>menu_item_vec_5</th>\n",
       "      <th>menu_item_vec_6</th>\n",
       "      <th>menu_item_vec_7</th>\n",
       "      <th>menu_item_vec_8</th>\n",
       "      <th>menu_item_vec_9</th>\n",
       "      <th>menu_item_vec_10</th>\n",
       "      <th>menu_item_vec_11</th>\n",
       "      <th>menu_item_vec_12</th>\n",
       "      <th>menu_item_vec_13</th>\n",
       "      <th>menu_item_vec_14</th>\n",
       "      <th>menu_item_vec_15</th>\n",
       "      <th>menu_item_vec_16</th>\n",
       "      <th>menu_item_vec_17</th>\n",
       "      <th>menu_item_vec_18</th>\n",
       "      <th>menu_item_vec_19</th>\n",
       "      <th>cuisines_vec_0</th>\n",
       "      <th>cuisines_vec_1</th>\n",
       "      <th>cuisines_vec_2</th>\n",
       "      <th>cuisines_vec_3</th>\n",
       "      <th>cuisines_vec_4</th>\n",
       "      <th>cuisines_vec_5</th>\n",
       "      <th>cuisines_vec_6</th>\n",
       "      <th>cuisines_vec_7</th>\n",
       "      <th>cuisines_vec_8</th>\n",
       "      <th>cuisines_vec_9</th>\n",
       "      <th>cuisines_vec_10</th>\n",
       "      <th>cuisines_vec_11</th>\n",
       "      <th>cuisines_vec_12</th>\n",
       "      <th>cuisines_vec_13</th>\n",
       "      <th>cuisines_vec_14</th>\n",
       "      <th>cuisines_vec_15</th>\n",
       "      <th>cuisines_vec_16</th>\n",
       "      <th>cuisines_vec_17</th>\n",
       "      <th>cuisines_vec_18</th>\n",
       "      <th>cuisines_vec_19</th>\n",
       "      <th>dish_liked_vec_0</th>\n",
       "      <th>dish_liked_vec_1</th>\n",
       "      <th>dish_liked_vec_2</th>\n",
       "      <th>dish_liked_vec_3</th>\n",
       "      <th>dish_liked_vec_4</th>\n",
       "      <th>dish_liked_vec_5</th>\n",
       "      <th>dish_liked_vec_6</th>\n",
       "      <th>dish_liked_vec_7</th>\n",
       "      <th>dish_liked_vec_8</th>\n",
       "      <th>dish_liked_vec_9</th>\n",
       "      <th>dish_liked_vec_10</th>\n",
       "      <th>dish_liked_vec_11</th>\n",
       "      <th>dish_liked_vec_12</th>\n",
       "      <th>dish_liked_vec_13</th>\n",
       "      <th>dish_liked_vec_14</th>\n",
       "      <th>dish_liked_vec_15</th>\n",
       "      <th>dish_liked_vec_16</th>\n",
       "      <th>dish_liked_vec_17</th>\n",
       "      <th>dish_liked_vec_18</th>\n",
       "      <th>dish_liked_vec_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jalsa</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[[Rated 2.0, RATED\\n  Its a restaurant near to...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Dum Biryani, Lunch Buffet, Masala Papad, Pane...</td>\n",
       "      <td>RATED\\n  Its a restaurant near to Banashankari...</td>\n",
       "      <td>10</td>\n",
       "      <td>2906</td>\n",
       "      <td>4.662083</td>\n",
       "      <td>its a restaurant near to banashankari bda. me ...</td>\n",
       "      <td>restaurant near banashankari bda along office ...</td>\n",
       "      <td>0.345060</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>-0.053563</td>\n",
       "      <td>-0.142821</td>\n",
       "      <td>-0.054958</td>\n",
       "      <td>-0.021470</td>\n",
       "      <td>-0.131860</td>\n",
       "      <td>-0.106416</td>\n",
       "      <td>0.075845</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>-0.025933</td>\n",
       "      <td>0.606136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.014075</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>-0.008531</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.010905</td>\n",
       "      <td>0.007609</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>-0.004918</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>-0.007041</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[[Rated 2.0, RATED\\n  I had a very bad experie...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Chicken Biryani, Chocolate Nirvana, Dum Birya...</td>\n",
       "      <td>RATED\\n  I had a very bad experience here.\\nI ...</td>\n",
       "      <td>14</td>\n",
       "      <td>4958</td>\n",
       "      <td>4.493304</td>\n",
       "      <td>i had a very bad experience here.\\ni don't kno...</td>\n",
       "      <td>bad experience know carte buffet worst gave co...</td>\n",
       "      <td>0.195731</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.602828</td>\n",
       "      <td>-0.031769</td>\n",
       "      <td>-0.063036</td>\n",
       "      <td>-0.030274</td>\n",
       "      <td>-0.018630</td>\n",
       "      <td>-0.168695</td>\n",
       "      <td>-0.074582</td>\n",
       "      <td>0.049469</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>-0.075545</td>\n",
       "      <td>0.409859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>-0.007676</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>-0.004245</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.001917</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.019428</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>-0.016491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[[Rated 1.0, RATED\\n  Cockroaches !! I Repeat ...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Cannelloni, Churros, Hot Chocolate, Minestron...</td>\n",
       "      <td>RATED\\n  Cockroaches !! I Repeat cockroaches!!...</td>\n",
       "      <td>20</td>\n",
       "      <td>6993</td>\n",
       "      <td>4.519108</td>\n",
       "      <td>cockroaches !! i repeat cockroaches!!bakasura ...</td>\n",
       "      <td>cockroach repeat cockroach bakasura disappoint...</td>\n",
       "      <td>0.166162</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.331411</td>\n",
       "      <td>-0.179015</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.081526</td>\n",
       "      <td>0.052696</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>0.051919</td>\n",
       "      <td>0.096816</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>-0.050901</td>\n",
       "      <td>0.188120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>-0.019702</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>-0.012497</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>-0.006153</td>\n",
       "      <td>-0.013886</td>\n",
       "      <td>-0.007933</td>\n",
       "      <td>-0.002460</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.003433</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>-0.002099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>1st Floor, Annakuteera, 3rd Stage, Banashankar...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>[[Rated 1.5, RATED\\n  The food was not satisfa...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Masala Dosa]</td>\n",
       "      <td>RATED\\n  The food was not satisfactory. Not on...</td>\n",
       "      <td>23</td>\n",
       "      <td>7708</td>\n",
       "      <td>4.539797</td>\n",
       "      <td>the food was not satisfactory. not one item se...</td>\n",
       "      <td>food satisfactory one item served could eaten ...</td>\n",
       "      <td>0.309284</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.541989</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>-0.120429</td>\n",
       "      <td>-0.329167</td>\n",
       "      <td>-0.045043</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>0.053153</td>\n",
       "      <td>0.119441</td>\n",
       "      <td>0.053072</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.185093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>-0.001145</td>\n",
       "      <td>-0.044709</td>\n",
       "      <td>-0.004473</td>\n",
       "      <td>-0.001355</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.043311</td>\n",
       "      <td>-0.013881</td>\n",
       "      <td>-0.043410</td>\n",
       "      <td>-0.044169</td>\n",
       "      <td>-0.005288</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>-0.011015</td>\n",
       "      <td>0.042738</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.032093</td>\n",
       "      <td>0.019955</td>\n",
       "      <td>-0.036016</td>\n",
       "      <td>-0.010618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grand Village</td>\n",
       "      <td>10, 3rd Floor, Lakshmi Associates, Gandhi Baza...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[[Rated 4.0, RATED\\n  Great service, overwhelm...</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Gol Gappe, Panipuri]</td>\n",
       "      <td>RATED\\n  Great service, overwhelming experienc...</td>\n",
       "      <td>2</td>\n",
       "      <td>651</td>\n",
       "      <td>5.252427</td>\n",
       "      <td>great service, overwhelming experience.\\n\\none...</td>\n",
       "      <td>great service overwhelming experience one kind...</td>\n",
       "      <td>0.447883</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>-0.022821</td>\n",
       "      <td>-0.090174</td>\n",
       "      <td>-0.110779</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>-0.052406</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>0.065407</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>-0.035731</td>\n",
       "      <td>0.668551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>-0.015820</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>-0.030901</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>-0.018324</td>\n",
       "      <td>-0.014460</td>\n",
       "      <td>-0.002780</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.013286</td>\n",
       "      <td>-0.006045</td>\n",
       "      <td>0.032203</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>-0.019635</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>-0.017556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                            address  \\\n",
       "0                  Jalsa  942, 21st Main Road, 2nd Stage, Banashankari, ...   \n",
       "1         Spice Elephant  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...   \n",
       "2        San Churro Cafe  1112, Next to KIMS Medical College, 17th Cross...   \n",
       "3  Addhuri Udupi Bhojana  1st Floor, Annakuteera, 3rd Stage, Banashankar...   \n",
       "4          Grand Village  10, 3rd Floor, Lakshmi Associates, Gandhi Baza...   \n",
       "\n",
       "   rate                                       reviews_list  menu_item  \\\n",
       "0   4.1  [[Rated 2.0, RATED\\n  Its a restaurant near to...  [Unknown]   \n",
       "1   4.1  [[Rated 2.0, RATED\\n  I had a very bad experie...  [Unknown]   \n",
       "2   3.8  [[Rated 1.0, RATED\\n  Cockroaches !! I Repeat ...  [Unknown]   \n",
       "3   3.7  [[Rated 1.5, RATED\\n  The food was not satisfa...  [Unknown]   \n",
       "4   3.8  [[Rated 4.0, RATED\\n  Great service, overwhelm...  [Unknown]   \n",
       "\n",
       "  cuisines                                         dish_liked  \\\n",
       "0       []  [Dum Biryani, Lunch Buffet, Masala Papad, Pane...   \n",
       "1       []  [Chicken Biryani, Chocolate Nirvana, Dum Birya...   \n",
       "2       []  [Cannelloni, Churros, Hot Chocolate, Minestron...   \n",
       "3       []                                      [Masala Dosa]   \n",
       "4       []                              [Gol Gappe, Panipuri]   \n",
       "\n",
       "                                    full_review_text  review_count  \\\n",
       "0  RATED\\n  Its a restaurant near to Banashankari...            10   \n",
       "1  RATED\\n  I had a very bad experience here.\\nI ...            14   \n",
       "2  RATED\\n  Cockroaches !! I Repeat cockroaches!!...            20   \n",
       "3  RATED\\n  The food was not satisfactory. Not on...            23   \n",
       "4  RATED\\n  Great service, overwhelming experienc...             2   \n",
       "\n",
       "   total_review_length  avg_word_length  \\\n",
       "0                 2906         4.662083   \n",
       "1                 4958         4.493304   \n",
       "2                 6993         4.519108   \n",
       "3                 7708         4.539797   \n",
       "4                  651         5.252427   \n",
       "\n",
       "                                  text_for_sentiment  \\\n",
       "0  its a restaurant near to banashankari bda. me ...   \n",
       "1  i had a very bad experience here.\\ni don't kno...   \n",
       "2  cockroaches !! i repeat cockroaches!!bakasura ...   \n",
       "3  the food was not satisfactory. not one item se...   \n",
       "4  great service, overwhelming experience.\\n\\none...   \n",
       "\n",
       "                                     text_for_topics  sentiment_textblob  \\\n",
       "0  restaurant near banashankari bda along office ...            0.345060   \n",
       "1  bad experience know carte buffet worst gave co...            0.195731   \n",
       "2  cockroach repeat cockroach bakasura disappoint...            0.166162   \n",
       "3  food satisfactory one item served could eaten ...            0.309284   \n",
       "4  great service overwhelming experience one kind...            0.447883   \n",
       "\n",
       "   sentiment_vader   topic_0   topic_1   topic_2   topic_3   topic_4  \\\n",
       "0           0.9996  0.482831 -0.053563 -0.142821 -0.054958 -0.021470   \n",
       "1           0.9996  0.602828 -0.031769 -0.063036 -0.030274 -0.018630   \n",
       "2           0.9997  0.331411 -0.179015  0.001051  0.081526  0.052696   \n",
       "3           0.9998  0.541989  0.015496 -0.120429 -0.329167 -0.045043   \n",
       "4           0.9856  0.220451 -0.022821 -0.090174 -0.110779 -0.016000   \n",
       "\n",
       "    topic_5   topic_6   topic_7   topic_8   topic_9  transformer_sentiment  \\\n",
       "0 -0.131860 -0.106416  0.075845  0.018076 -0.025933               0.606136   \n",
       "1 -0.168695 -0.074582  0.049469  0.012818 -0.075545               0.409859   \n",
       "2  0.054141  0.051919  0.096816  0.014956 -0.050901               0.188120   \n",
       "3 -0.000278  0.053153  0.119441  0.053072  0.128186               0.185093   \n",
       "4 -0.052406 -0.034730  0.065407  0.034503 -0.035731               0.668551   \n",
       "\n",
       "   menu_item_vec_0  menu_item_vec_1  menu_item_vec_2  menu_item_vec_3  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   menu_item_vec_4  menu_item_vec_5  menu_item_vec_6  menu_item_vec_7  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   menu_item_vec_8  menu_item_vec_9  menu_item_vec_10  menu_item_vec_11  \\\n",
       "0              0.0              0.0               0.0               0.0   \n",
       "1              0.0              0.0               0.0               0.0   \n",
       "2              0.0              0.0               0.0               0.0   \n",
       "3              0.0              0.0               0.0               0.0   \n",
       "4              0.0              0.0               0.0               0.0   \n",
       "\n",
       "   menu_item_vec_12  menu_item_vec_13  menu_item_vec_14  menu_item_vec_15  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   menu_item_vec_16  menu_item_vec_17  menu_item_vec_18  menu_item_vec_19  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   cuisines_vec_0  cuisines_vec_1  cuisines_vec_2  cuisines_vec_3  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   cuisines_vec_4  cuisines_vec_5  cuisines_vec_6  cuisines_vec_7  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   cuisines_vec_8  cuisines_vec_9  cuisines_vec_10  cuisines_vec_11  \\\n",
       "0             0.0             0.0              0.0              0.0   \n",
       "1             0.0             0.0              0.0              0.0   \n",
       "2             0.0             0.0              0.0              0.0   \n",
       "3             0.0             0.0              0.0              0.0   \n",
       "4             0.0             0.0              0.0              0.0   \n",
       "\n",
       "   cuisines_vec_12  cuisines_vec_13  cuisines_vec_14  cuisines_vec_15  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   cuisines_vec_16  cuisines_vec_17  cuisines_vec_18  cuisines_vec_19  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   dish_liked_vec_0  dish_liked_vec_1  dish_liked_vec_2  dish_liked_vec_3  \\\n",
       "0         -0.000139         -0.014075          0.016368         -0.008531   \n",
       "1          0.010823         -0.001898         -0.003000         -0.007676   \n",
       "2         -0.003370         -0.013840          0.005028          0.002750   \n",
       "3          0.023452         -0.001145         -0.044709         -0.004473   \n",
       "4          0.026897         -0.015820          0.000905         -0.030901   \n",
       "\n",
       "   dish_liked_vec_4  dish_liked_vec_5  dish_liked_vec_6  dish_liked_vec_7  \\\n",
       "0         -0.002681         -0.002833         -0.004693         -0.005675   \n",
       "1         -0.004414          0.001336          0.002587          0.007630   \n",
       "2         -0.019702         -0.003763         -0.012497          0.021039   \n",
       "3         -0.001355          0.004533          0.043311         -0.013881   \n",
       "4          0.003124          0.001516          0.026496         -0.018324   \n",
       "\n",
       "   dish_liked_vec_8  dish_liked_vec_9  dish_liked_vec_10  dish_liked_vec_11  \\\n",
       "0         -0.010905          0.007609           0.004142           0.018541   \n",
       "1         -0.004245         -0.001513          -0.001917          -0.000603   \n",
       "2         -0.006153         -0.013886          -0.007933          -0.002460   \n",
       "3         -0.043410         -0.044169          -0.005288           0.006859   \n",
       "4         -0.014460         -0.002780          -0.005007          -0.013286   \n",
       "\n",
       "   dish_liked_vec_12  dish_liked_vec_13  dish_liked_vec_14  dish_liked_vec_15  \\\n",
       "0           0.008585          -0.004918           0.002574          -0.000303   \n",
       "1          -0.019428           0.005711           0.014509           0.005603   \n",
       "2           0.003207           0.006325           0.011423          -0.002227   \n",
       "3           0.000436          -0.011015           0.042738           0.000532   \n",
       "4          -0.006045           0.032203           0.008210          -0.019635   \n",
       "\n",
       "   dish_liked_vec_16  dish_liked_vec_17  dish_liked_vec_18  dish_liked_vec_19  \n",
       "0          -0.004193          -0.007041           0.003208           0.000806  \n",
       "1          -0.006174          -0.005010           0.010610          -0.016491  \n",
       "2          -0.004967          -0.003433           0.000840          -0.002099  \n",
       "3           0.032093           0.019955          -0.036016          -0.010618  \n",
       "4           0.014608           0.000582           0.042274          -0.017556  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New shape: (45187, 86)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def create_food_embeddings_simple_v2(df: pd.DataFrame, \n",
    "                                     list_cols: list, \n",
    "                                     vector_size: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A simplified and direct Word2Vec pipeline that is robust to both\n",
    "    Python lists and NumPy arrays in the input columns.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Simplified Feature Engineering with Custom Food Embeddings (v2) ---\")\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # --- Step 1: Build the Giant Unified Corpus ---\n",
    "    logger.info(\"Step 1: Building a unified corpus...\")\n",
    "    corpus = []\n",
    "    for col in list_cols:\n",
    "        for item_list in df_out[col]:\n",
    "            # This check is important to handle different empty types\n",
    "            if hasattr(item_list, '__len__') and len(item_list) > 0:\n",
    "                clean_list = [item for item in item_list if str(item).lower() != 'unknown']\n",
    "                if clean_list:\n",
    "                    corpus.append(clean_list)\n",
    "        \n",
    "    if not corpus:\n",
    "        logger.error(\"Corpus is empty. Aborting.\"); return df_out\n",
    "        \n",
    "    logger.info(f\"Unified corpus created with {len(corpus):,} total documents.\")\n",
    "\n",
    "    # --- Step 2: Train the custom Word2Vec model ---\n",
    "    logger.info(f\"Step 2: Training a custom Word2Vec model...\")\n",
    "    w2v_model = Word2Vec(sentences=corpus, vector_size=vector_size, window=5, min_count=3, workers=-1, sg=1)\n",
    "    logger.success(\"Custom Word2Vec model trained successfully.\")\n",
    "\n",
    "    # --- Step 3: Create a function to vectorize a list of items ---\n",
    "    def get_average_vector(items_list):\n",
    "        # --- THE FIX IS HERE ---\n",
    "        # Instead of 'if not items_list', we explicitly check the length.\n",
    "        # This works for both Python lists and NumPy arrays.\n",
    "        if not hasattr(items_list, '__len__') or len(items_list) == 0:\n",
    "            return np.zeros(vector_size)\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        vectors = [w2v_model.wv[item] for item in items_list if item in w2v_model.wv]\n",
    "        if not vectors: return np.zeros(vector_size)\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    # --- Step 4: Apply the vectorization to each column ---\n",
    "    logger.info(\"Step 3: Vectorizing each column...\")\n",
    "    for col in tqdm(list_cols, desc=\"Creating Embedding Features\"):\n",
    "        vectors = df_out[col].apply(get_average_vector) # This will now work\n",
    "        vec_df = pd.DataFrame(vectors.tolist(), index=df_out.index)\n",
    "        vec_df.columns = [f\"{col}_vec_{i}\" for i in range(vector_size)]\n",
    "        df_out = pd.concat([df_out, vec_df], axis=1)\n",
    "\n",
    "    logger.success(\"All specified columns have been converted into embedding features.\")\n",
    "    return df_out\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# ... (your execution code remains the same) ...\n",
    "cols_to_embed = ['menu_item', 'cuisines', 'dish_liked']\n",
    "df_with_embeddings = create_food_embeddings_simple_v2(df_nlp_final, \n",
    "                                                     list_cols=cols_to_embed, \n",
    "                                                     vector_size=20)\n",
    "\n",
    "# --- Verification ---\n",
    "display(df_with_embeddings.head())\n",
    "print(\"\\nNew shape:\", df_with_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6998c20c-ad57-4ca9-9eed-171cde58ad4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:23:48.503569Z",
     "iopub.status.busy": "2025-09-17T15:23:48.503285Z",
     "iopub.status.idle": "2025-09-17T15:23:52.661636Z",
     "shell.execute_reply": "2025-09-17T15:23:52.660811Z",
     "shell.execute_reply.started": "2025-09-17T15:23:48.503551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 20:53:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m--- Exporting Final NLP Feature Set ((45187, 86)) ---\u001b[0m\n",
      "\u001b[32m2025-09-17 20:53:52\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1mFinal NLP features saved successfully to '../data/processed/zomato_nlp_features_final.parquet'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# df_with_embeddings is your final DataFrame from the previous step\n",
    "\n",
    "# --- FINAL EXPORT ---\n",
    "FINAL_NLP_PATH = \"../data/processed/zomato_nlp_features_final.parquet\"\n",
    "logger.info(f\"--- Exporting Final NLP Feature Set ({df_with_embeddings.shape}) ---\")\n",
    "\n",
    "try:\n",
    "    df_with_embeddings.to_parquet(FINAL_NLP_PATH, index=False)\n",
    "    logger.success(f\"Final NLP features saved successfully to '{FINAL_NLP_PATH}'\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save final NLP features: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b76df1-f7ee-4fa2-9078-2c05f4e7c3b3",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Final NLP Export\n",
    "\n",
    "This notebook has successfully executed a comprehensive, multi-phase NLP feature engineering pipeline. We have progressed from basic text statistics to classic sentiment analysis, topic modeling, and finally, state-of-the-art word embeddings.\n",
    "\n",
    "The final, enriched NLP DataFrame, containing our primary keys, target variable, and all newly engineered features, is now ready for export.\n",
    "\n",
    "**Final Exported File:**\n",
    "-   `zomato_nlp_features_final.parquet`\n",
    "\n",
    "This file will serve as a critical input to our final modeling notebook, where these rich features will be merged with our tabular and geospatial data to build the ultimate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0512fb-45d0-4bd0-94cf-aba47edae88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
